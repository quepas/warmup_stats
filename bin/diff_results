#!/usr/bin/python3

# Copyright (c) 2017 King's College London
# created by the Software Development Team <http://soft-dev.org/>
#
# The Universal Permissive License (UPL), Version 1.0
#
# Subject to the condition set forth below, permission is hereby granted to any
# person obtaining a copy of this software, associated documentation and/or
# data (collectively the "Software"), free of charge and under any and all
# copyright rights in the Software, and any and all patent rights owned or
# freely licensable by each licensor hereunder covering either (i) the
# unmodified Software as contributed to or provided by such licensor, or (ii)
# the Larger Works (as defined below), to deal in both
#
# (a) the Software, and
# (b) any piece of software and/or hardware listed in the lrgrwrks.txt file if
# one is included with the Software (each a "Larger Work" to which the Software
# is contributed by such licensors),
#
# without restriction, including without limitation the rights to copy, create
# derivative works of, display, perform, and distribute the Software and make,
# use, sell, offer for sale, import, export, have made, and have sold the
# Software and the Larger Work(s), and to sublicense the foregoing rights on
# either these or other terms.
#
# This license is subject to the following condition: The above copyright
# notice and either this complete permission notice or at a minimum a reference
# to the UPL must be included in all copies or substantial portions of the
# Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

"""
Plot a chart describing the effect of re-running an experiment with fewer iterations.
MUST be run after generate_truncated_json.
"""

import collections
import math
import os
import sys

# R packages are stored relative to the top-level of the repo.
if ('R_LIBS_USER' not in os.environ or 'rlibs' not in os.environ['R_LIBS_USER']):
    os.environ['R_LIBS_USER'] = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))),
                                             'work', 'rlibs')
    args = [sys.executable]
    args.extend(sys.argv)
    os.execv(sys.executable, args)

# We use a custom install of rpy2, relative to the top-level of the repo.
sys.path.insert(0, os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))),
                                'work', 'pylibs'))

import argparse
import json
import numpy

import rpy2
import rpy2.interactive.packages
import rpy2.robjects

sys.path.insert(0, os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from warmup.krun_results import parse_krun_file_with_changepoints
from warmup.latex import end_document, end_longtable, end_table, escape
from warmup.latex import get_latex_symbol_map, preamble
from warmup.latex import start_longtable, start_table, STYLE_SYMBOLS
from warmup.summary_statistics import BLANK_CELL, collect_summary_statistics
from warmup.summary_statistics import convert_to_latex, write_html_table

DESCRIPTION = lambda fname: """
Diff two Krun results files. Input files to this script should already have
outliers and changepoints marked (i.e. the mark_outliers_in_json and
mark_changepoints_in_json scripts should already have been run). Output
can be in HTML or LaTeX. A JSON file containing a raw diff is dumped to disk.

Example usage (input Krun results files, output HTML):

    $ python %s --json diff_summary.json --input-results before.json.bz2 after.json.bz2 --html diff.html

Example usage (input JSON summary file, output LaTeX):

    $ python %s --input-summary diff_summary.json --tex diff.tex
"""

ALPHA = 0.01  # Significance level.
CI_MINIUM_SIGNIFICANT_NARROWING = 0.0001 # In seconds
CATEGORIES = ['warmup', 'slowdown', 'flat', 'no steady state']
MCI = rpy2.interactive.packages.importr('MultinomialCI')
# List indices (used in favour of dictionary keys).
CLASSIFICATIONS = 0  # Indices for top-level summary lists.
STEADY_ITER = 1
STEADY_ITER_VAR = 2
STEADY_STATE_TIME = 3
STEADY_STATE_TIME_VAR = 4
INTERSECTION = 5
SAME = 0  # Indices for nested lists.
DIFFERENT = 1
BETTER = 2
WORSE = 3
SKIPPED_BEFORE = 0
SKIPPED_AFTER = 1
# Dictionary keys
BEFORE = 'before'
AFTER = 'after'
CLASSIFIER = 'classifier'
DIFF = 'diff'
SKIPPED = 'skipped'
# LaTeX output.
TITLE = 'Summary of benchmark classifications'
TABLE_FORMAT = ('ll@{\hspace{0cm}}ll@{\hspace{0cm}}r@{\hspace{.4cm}}r@{\hspace{.4cm}}r@{\hspace{.4cm}}r@{\hspace{.4cm}}'
                'l@{\hspace{.4cm}}ll@{\hspace{.4cm}}r@{\hspace{.4cm}}r@{\hspace{.4cm}}rr@{\hspace{.4cm}}r')
TABLE_HEADINGS_START1 = '\\multicolumn{1}{c}{\\multirow{2}{*}{}}&'
TABLE_HEADINGS_START2 = '&'
TABLE_HEADINGS1 = ('&&\\multicolumn{1}{c}{} &\\multicolumn{1}{c}{Steady} &\\multicolumn{1}{c}{Steady iter.} '
                   '&\\multicolumn{1}{c}{Steady} &\\multicolumn{1}{c}{Steady} &\\multicolumn{1}{c}{Steady perf.}')
TABLE_HEADINGS2 = ('&&\\multicolumn{1}{c}{Class.} &\\multicolumn{1}{c}{iter (\#)} &\\multicolumn{1}{c}{variation (s)} '
                   '&\\multicolumn{1}{c}{iter (s)} &\\multicolumn{1}{c}{perf (s)} &\\multicolumn{1}{c}{variation (s)}')

JSON_VERSION_NUMBER = '2'


def fatal(message):
    print(message)
    sys.exit(1)


def legend():
    table = """\\begin{tabular}{l|l|l|l}
\\multicolumn{1}{c}{%s} & \\multicolumn{1}{c}{%s} & \\multicolumn{1}{c}{%s} & \\multicolumn{1}{c}{%s} \\\\
\\end{tabular}
""" % (colour_tex_cell(BETTER, 'improved'),
       colour_tex_cell(WORSE, 'worsened'),
       colour_tex_cell(DIFFERENT, 'different'),
       colour_tex_cell(SAME, 'unchanged'))
    return '\\textbf{Diff against previous results:} ' + table


def do_intervals_differ(p1, p2):
    """Given two IQRs or CIs return True if they do NOT overlap."""

    assert p1[1] >= p1[0] and p2[1] >= p2[0]
    return p1[1] < p2[0] or p2[1] < p1[0]


def does_interval_narrow(p1, p2):
    """Return True if the second interval is narrower than the first."""

    assert p1[1] >= p1[0] and p2[1] >= p2[0]
    # Sometimes there is no variation in a dataset (more likely for steady iter
    # than steady perf, for obvious reasons). So, we check for special cases,
    # rather than causing a divide by zero.
    diff1, diff2 = p1[1] - p1[0], p2[1] - p2[0]
    if diff1 == 0 and diff2 == 0:
        return SAME
    if diff1 == 0 and diff2 > 0:
        return WORSE
    ratio = float(diff2) / float(diff1)
    if ratio == 1.0:
        return SAME
    elif ratio < 1.0:
        return BETTER
    return WORSE


def do_mean_cis_differ(mean1, ci1, mean2, ci2):
    """Given two means +/- CIs return True if they do NOT overlap."""

    assert ci1 >= 0.0 and ci2 >= 0.0, 'Found negative confidence interval from bootstrapping.'
    x1 = mean1 - ci1
    y1 = mean1 + ci1
    x2 = mean2 - ci2
    y2 = mean2 + ci2
    return do_intervals_differ((x1, y1), (x2, y2))


def does_ci_narrow(mean1, ci1, mean2, ci2):
    """Return True if the second interval is narrower than the first."""

    assert ci1 >= 0.0 and ci2 >= 0.0, 'Found negative confidence interval from bootstrapping.'
    if abs(ci1 - ci2) < CI_MINIUM_SIGNIFICANT_NARROWING:
        return SAME
    x1 = mean1 - ci1
    y1 = mean1 + ci1
    x2 = mean2 - ci2
    y2 = mean2 + ci2
    return does_interval_narrow((x1, y1), (x2, y2))


def all_flat(classifications):
    """Return True if all pexecs in a detailed classification dict are 'flat'."""

    return (classifications['warmup'] == 0 and classifications['slowdown'] == 0
            and classifications['no steady state'] == 0)


def all_nss(classifications):
    """Return True if all pexecs in a detailed classification dict are 'no steady state'."""

    return (classifications['warmup'] == 0 and classifications['slowdown'] == 0 and
            classifications['flat'] == 0)


def any_nss(classifications):
    """Return True if any pexec in a detailed classification dict is 'no steady state'."""

    return classifications['no steady state'] > 0


def _rewrite_key(key, diff_vms):
    """Rewrite a bench:vm:language key.
    Takes as arguments one key, and a list of two VM names, that the user wants
    to compare - ['VM1', 'VM2']. Rewrites the key from 'bench:vm:language' to
    'bench:VM1 vs. VM2:language'.

    By default, the differ compares all VMs that appear in one results file
    against results from the same the same VM in a second file. When the user
    wants to compare one VM against another, this renaming fools the differ
    into thinking that the two different VMs are the same. We choose the name
    'VM1 vs. VM2' since that is the text we wish to appear in the output tables.
    """

    split = key.split(':')
    combined_vm = ' vs. '.join(diff_vms)
    return ':'.join([split[0], combined_vm, split[2]])


def diff(before_file, after_file, summary_filename, diff_vms=[]):
    """Diff results in before_file and after_file."""

    classifiers = dict()
    before_results = None
    # In the JSON dump, we need the diff, and  the original summaries of the
    # before / after results, so that they can be written into a LaTeX table.
    summary = {DIFF: dict(), SKIPPED: [[], []], BEFORE: None, AFTER: None, CLASSIFIER: None}
    print('Loading %s.' % before_file)
    classifiers[BEFORE], before_results = parse_krun_file_with_changepoints([before_file])
    print('Loading %s.' % after_file)
    classifiers[AFTER], after_results = parse_krun_file_with_changepoints([after_file])
    assert len(before_results.keys()) == 1, 'Expected one machine per results file.'
    assert len(after_results.keys()) == 1, 'Expected one machine per results file.'
    assert list(before_results.keys())[0] == list(after_results.keys())[0], 'Expected results to be from same machine.'
    machine = list(before_results.keys())[0]
    # Special case: the user wants to diff one VM against another (by default,
    # we diff each VM against itself, for every VM that appears in both the
    # before and after data). If the user wants to diff one VM against another,
    # then we rename the "before" and "after" VMs to these special names, and
    # rename them back before presenting the diff results back to the user. This
    # is an unpleasant hack, but since the diff information is baked into the
    # structure of the summary diff, improving this code (and the LaTeX / HTML
    # output code) would be a significant task.
    if diff_vms:
        before_vm, after_vm = diff_vms
        found_before_vm, found_after_vm =  False, False
        for dtype in before_results[machine]:
            if isinstance(before_results[machine][dtype], collections.Iterable):
                for key in before_results[machine][dtype]:
                    if before_vm in key:
                        found_before_vm = True
                        new_key = _rewrite_key(key, diff_vms)
                        before_results[machine][dtype][new_key] = before_results[machine][dtype].pop(key)
        if not found_before_vm:
             fatal('Could not find requested VM in results data: ' + before_vm)
        for dtype in after_results[machine]:
            if isinstance(after_results[machine][dtype], collections.Iterable):
                for key in after_results[machine][dtype]:
                    if after_vm in key:
                        found_after_vm = True
                        new_key = _rewrite_key(key, diff_vms)
                        after_results[machine][dtype][new_key] = after_results[machine][dtype].pop(key)
        if not found_after_vm:
             fatal('Could not find requested VM in results data: ' + after_vm)
    summary[BEFORE] = collect_summary_statistics(before_results,
                                                 classifiers[BEFORE]['delta'], classifiers[BEFORE]['steady'])
    summary[AFTER] = collect_summary_statistics(after_results,
                                                classifiers[AFTER]['delta'], classifiers[AFTER]['steady'])
    for key in classifiers[BEFORE]:
        assert classifiers[BEFORE][key] == classifiers[AFTER][key], \
            'Results files generated with different values for %s' % key
    summary[CLASSIFIER] = classifiers[AFTER]
    # Generate CIs for DEFAULT_ITER classification data.
    before_class_cis = dict()
    for key in before_results[machine]['classifications']:
        if len(before_results[machine]['classifications'][key]) == 0:  # Skipped benchmark.
            continue
        class_counts = [before_results[machine]['classifications'][key].count(category) for category in CATEGORIES]
        before_class_cis[key] = numpy.array(MCI.multinomialCI(rpy2.robjects.FloatVector(class_counts), ALPHA))
    for key in after_results[machine]['classifications']:
        if len(after_results[machine]['classifications'][key]) == 0:  # Skipped benchmark.
            continue
        if key in before_results[machine]['classifications']:
            bench, vm = key.split(':')[:-1]
            if vm not in summary[DIFF]:
                summary[DIFF][vm] = dict()
            summary[DIFF][vm][bench] = [None, None, None, None, None, None]
    for key in after_results[machine]['classifications']:
        bench, vm = key.split(':')[:-1]
        # Deal with skipped benchmarks.
        if (not key in before_results[machine]['classifications']
            or len(before_results[machine]['classifications'][key]) == 0):
            summary[SKIPPED][SKIPPED_BEFORE].append((bench, vm))
            continue
        elif len(after_results[machine]['classifications'][key]) == 0:
            summary[SKIPPED][SKIPPED_AFTER].append((bench, vm))
            continue
        # Classifications are available, whether or not summary statistics can be generated.
        trunc_cat = [summary[AFTER]['machines'][machine][vm][bench]['process_executons'][p]['classification'] \
                     for p in range(len(summary[AFTER]['machines'][machine][vm][bench]['process_executons']))]
        trunc_counts = [trunc_cat.count(category) for category in CATEGORIES]
        after_class_cis = numpy.array(MCI.multinomialCI(rpy2.robjects.FloatVector(trunc_counts), ALPHA))
        sample = summary[AFTER]['machines'][machine][vm][bench]
        base_case = summary[BEFORE]['machines'][machine][vm][bench]
        for category in CATEGORIES:
            cat_index = CATEGORIES.index(category)
            if do_intervals_differ(before_class_cis[key][cat_index], after_class_cis[cat_index]):
                if (sample['detailed_classification']['warmup'] + sample['detailed_classification']['flat'] >
                        base_case['detailed_classification']['warmup'] + base_case['detailed_classification']['flat']):
                    summary[DIFF][vm][bench][CLASSIFICATIONS] = BETTER
                    break
                elif (sample['detailed_classification']['no steady state'] + sample['detailed_classification']['slowdown'] >
                        base_case['detailed_classification']['no steady state'] + base_case['detailed_classification']['slowdown']):
                    summary[DIFF][vm][bench][CLASSIFICATIONS] = WORSE
                    break
                else:
                    summary[DIFF][vm][bench][CLASSIFICATIONS] = DIFFERENT
                    break
        else:
            summary[DIFF][vm][bench][CLASSIFICATIONS] = SAME
        # If the CIs did not overlap, but the ONLY difference is in the number
        # of warmups / flats, we say the results were the same (because we see
        # warmups / flats are the same case).
        if summary[DIFF][vm][bench][CLASSIFICATIONS] != SAME and \
                base_case['detailed_classification']['slowdown'] == sample['detailed_classification']['slowdown'] and \
                base_case['detailed_classification']['no steady state'] == sample['detailed_classification']['no steady state']:
            summary[DIFF][vm][bench][CLASSIFICATIONS] = SAME
        # If the CIs do overlap, but the classification has moved from bad
        # inconsistent to good inconsistent, then we say the result was better.
        if summary[DIFF][vm][bench][CLASSIFICATIONS] == SAME and \
                base_case['detailed_classification']['no steady state'] > 0 and \
                sample['detailed_classification']['no steady state'] == 0:
            summary[DIFF][vm][bench][CLASSIFICATIONS] = BETTER
        # That completes the category data. The remaining logic deals with the
        # numerical data (time to reach a steady state, steady state time per
        # iteration), and produces an overall classification for this benchmark.
        # Case 1) All flat.
        if (all_flat(sample['detailed_classification']) and all_flat(base_case['detailed_classification'])):
            summary[DIFF][vm][bench][STEADY_ITER] = SAME
            if base_case['steady_state_time_ci'] is None:
                summary[DIFF][vm][bench][STEADY_STATE_TIME] = DIFFERENT
            elif do_mean_cis_differ(base_case['steady_state_time'], base_case['steady_state_time_ci'],
                                    sample['steady_state_time'], sample['steady_state_time_ci']):
                if sample['steady_state_time'] < base_case['steady_state_time']:
                    summary[DIFF][vm][bench][STEADY_STATE_TIME] = BETTER
                else:
                    summary[DIFF][vm][bench][STEADY_STATE_TIME] = WORSE
            else:
                summary[DIFF][vm][bench][STEADY_STATE_TIME] = SAME
                var = does_ci_narrow(base_case['steady_state_time'], base_case['steady_state_time_ci'],
                                     sample['steady_state_time'], sample['steady_state_time_ci'])
                summary[DIFF][vm][bench][STEADY_STATE_TIME_VAR] = var
        # Case 2) One ALL FLAT, one not.
        elif (all_flat(sample['detailed_classification']) or all_flat(base_case['detailed_classification'])):
            if (any_nss(sample['detailed_classification']) or any_nss(base_case['detailed_classification'])):
                summary[DIFF][vm][bench][STEADY_ITER] = DIFFERENT
            elif (all_flat(base_case['detailed_classification']) and
                  do_intervals_differ((1.0, 1.0), sample['steady_state_iteration_iqr'])):
                if sample['steady_state_iteration'] < base_case['steady_state_iteration']:
                    summary[DIFF][vm][bench][STEADY_ITER] = BETTER
                else:
                    summary[DIFF][vm][bench][STEADY_ITER] = WORSE
            elif (all_flat(sample['detailed_classification']) and
                  do_intervals_differ((1.0, 1.0), base_case['steady_state_iteration_iqr'])):
                if sample['steady_state_iteration'] < base_case['steady_state_iteration']:
                    summary[DIFF][vm][bench][STEADY_ITER] = BETTER
                else:
                    summary[DIFF][vm][bench][STEADY_ITER] = WORSE
            else:
                summary[DIFF][vm][bench][STEADY_ITER] = SAME
            if (any_nss(sample['detailed_classification']) or any_nss(base_case['detailed_classification'])):
                summary[DIFF][vm][bench][STEADY_STATE_TIME] = DIFFERENT
            elif do_mean_cis_differ(base_case['steady_state_time'], base_case['steady_state_time_ci'],
                                    sample['steady_state_time'], sample['steady_state_time_ci']):
                if sample['steady_state_time'] < base_case['steady_state_time']:
                    summary[DIFF][vm][bench][STEADY_STATE_TIME] = BETTER
                else:
                    summary[DIFF][vm][bench][STEADY_STATE_TIME] = WORSE
            else:
                summary[DIFF][vm][bench][STEADY_STATE_TIME] = SAME
                var = does_ci_narrow(base_case['steady_state_time'], base_case['steady_state_time_ci'],
                                     sample['steady_state_time'], sample['steady_state_time_ci'])
                summary[DIFF][vm][bench][STEADY_STATE_TIME_VAR] = var
        # Case 3) One contains an NSS (therefore no steady iter / perf available).
        elif (any_nss(sample['detailed_classification']) or any_nss(base_case['detailed_classification'])):
            pass
        # Case 4) All three measures should be available in both the DEFAULT_ITER and last_iter cases.
        else:
            # If n_pexecs is small, and the steady_iters are all identical,
            # we sometimes get odd IQRs like [7.000000000000001, 7.0], so
            # deal with this as a special case to avoid triggering the assertion
            # in do_intervals_differ.
            if len(set(sample['steady_state_iteration_list'])) == 1:
                fake_iqr = (float(sample['steady_state_iteration_list'][0]), float(sample['steady_state_iteration_list'][0]))
                if do_intervals_differ(base_case['steady_state_iteration_iqr'], fake_iqr):
                    if sample['steady_state_iteration'] < base_case['steady_state_iteration']:
                        summary[DIFF][vm][bench][STEADY_ITER] = BETTER
                    else:
                        summary[DIFF][vm][bench][STEADY_ITER] = WORSE
                else:
                    summary[DIFF][vm][bench][STEADY_ITER] = SAME
                summary[DIFF][vm][bench][STEADY_ITER_VAR] = SAME
            elif do_intervals_differ(base_case['steady_state_iteration_iqr'],
                                     sample['steady_state_iteration_iqr']):
                if sample['steady_state_iteration'] < base_case['steady_state_iteration']:
                    summary[DIFF][vm][bench][STEADY_ITER] = BETTER
                else:
                    summary[DIFF][vm][bench][STEADY_ITER] = WORSE
                var = does_interval_narrow(base_case['steady_state_iteration_iqr'], sample['steady_state_iteration_iqr'])
                summary[DIFF][vm][bench][STEADY_ITER_VAR] = var
            else:
                summary[DIFF][vm][bench][STEADY_ITER] = SAME
                var = does_interval_narrow(base_case['steady_state_iteration_iqr'], sample['steady_state_iteration_iqr'])
                summary[DIFF][vm][bench][STEADY_ITER_VAR] = var
            if do_mean_cis_differ(base_case['steady_state_time'], base_case['steady_state_time_ci'],
                                  sample['steady_state_time'], sample['steady_state_time_ci']):
                if sample['steady_state_time'] < base_case['steady_state_time']:
                    summary[DIFF][vm][bench][STEADY_STATE_TIME] = BETTER
                else:
                    summary[DIFF][vm][bench][STEADY_STATE_TIME] = WORSE
            else:
                summary[DIFF][vm][bench][STEADY_STATE_TIME] = SAME
            var = does_ci_narrow(base_case['steady_state_time'], base_case['steady_state_time_ci'],
                                 sample['steady_state_time'], sample['steady_state_time_ci'])
            summary[DIFF][vm][bench][STEADY_STATE_TIME_VAR] = var
        # Was the benchmark better or worse overall?
        if not (BETTER in summary[DIFF][vm][bench] or WORSE in summary[DIFF][vm][bench] or
                DIFFERENT in summary[DIFF][vm][bench]):
            summary[DIFF][vm][bench][INTERSECTION] = SAME
        elif BETTER in summary[DIFF][vm][bench] and not WORSE in summary[DIFF][vm][bench]:
            summary[DIFF][vm][bench][INTERSECTION] = BETTER
        elif WORSE in summary[DIFF][vm][bench] and not BETTER in summary[DIFF][vm][bench]:
            summary[DIFF][vm][bench][INTERSECTION] = WORSE
        else:
            summary[DIFF][vm][bench][INTERSECTION] = DIFFERENT
    with open(summary_filename, 'w') as fd:
        json.dump(summary, fd, ensure_ascii=True, indent=4)
        print('Saved: %s' % summary_filename)
    return summary


def colour_tex_cell(result, text):
    """Colour a table cell containing `text` according to `result`."""

    assert result in (None, SAME, DIFFERENT, BETTER, WORSE)
    if not text or result is None or result == SAME:
        return text
    if result == BETTER:
        colour = 'lightgreen'
    elif result == WORSE:
        colour = 'lightred'
    else:
        colour = 'lightyellow'
    return '\\cellcolor{%s!25}{%s}' % (colour, text)


def write_latex_table(machine, all_benchs, summary, diff, skipped, tex_file, num_splits,
                      with_preamble=False, longtable=False, diff_vms=[]):
    """Write a tex table to disk"""

    num_benchmarks = len(all_benchs)
    all_vms = sorted(summary.keys())
    num_vms = len(summary)

    # decide how to lay out the splits
    num_vms_rounded = int(math.ceil(num_vms / float(num_splits)) * num_splits)
    vms_per_split = int(num_vms_rounded / float(num_splits))
    splits = [[] for x in range(num_splits)]
    vm_num = 0
    split_idx = 0
    for vm_idx in range(num_vms_rounded):
        if vm_idx < len(all_vms):
            vm = all_vms[vm_idx]
        else:
            vm = None
        splits[split_idx].append(vm)
        vm_num += 1
        if vm_num % vms_per_split == 0:
            split_idx += 1

    with open(tex_file, 'w') as fp:
        if with_preamble:
            fp.write(preamble(TITLE))
            if diff_vms:
                fp.write('\\centering{%%\n\\Large{\\textbf{%s vs. %s}}%%\n}\n\\\\\n~\\\\\n\n'
                         % (diff_vms[0], diff_vms[1]))
            legends = get_latex_symbol_map() + ' \\\\ ' + legend()
            fp.write('\\centering %s' % legends)
            fp.write('\n\n\n')
            if not longtable:
                fp.write('\\begin{landscape}\n')
                fp.write('\\begin{table*}[hptb]\n')
                fp.write('\\vspace{.8cm}\n')
                fp.write('\\begin{adjustbox}{totalheight=12.4cm}\n')
        # Emit table header.
        heads1 = TABLE_HEADINGS_START1 + '&'.join([TABLE_HEADINGS1] * num_splits)
        heads2 = TABLE_HEADINGS_START2 + '&'.join([TABLE_HEADINGS2] * num_splits)
        heads = '%s\\\\%s' % (heads1, heads2)
        if longtable:
            fp.write(start_longtable(TABLE_FORMAT, heads))
        else:
            fp.write(start_table(TABLE_FORMAT, heads))
        split_row_idx = 0
        for row_vms in zip(*splits):
            bench_idx = 0
            skipped_before = [b for (b, v) in skipped[SKIPPED_BEFORE] if v == row_vms[0]]
            skipped_after = [b for (b, v) in skipped[SKIPPED_AFTER] if v == row_vms[0]]
            for bench in sorted(all_benchs + skipped_before + skipped_after):
                row = []
                for vm in row_vms:
                    if vm is None:
                        continue # no more results
                    try:
                        this_summary = summary[vm][bench]
                    except KeyError:
                        if bench in skipped_before or bench in skipped_after:
                            classification = '\\emph{Skipped}'
                        else:
                            classification = ''
                        last_cpt = BLANK_CELL
                        time_steady = BLANK_CELL
                        last_mean = BLANK_CELL
                        steady_iter_var = BLANK_CELL
                        steady_time_var = BLANK_CELL
                    else:
                        if vm in diff and bench in diff[vm]:
                            classification = colour_tex_cell(diff[vm][bench][CLASSIFICATIONS], this_summary['style'])
                            last_cpt = colour_tex_cell(diff[vm][bench][STEADY_ITER], this_summary['last_cpt'])
                            steady_iter_var = colour_tex_cell(diff[vm][bench][STEADY_ITER_VAR], this_summary['steady_iter_var'])
                            time_steady = colour_tex_cell(diff[vm][bench][STEADY_ITER], this_summary['time_to_steady_state'])
                            last_mean = colour_tex_cell(diff[vm][bench][STEADY_STATE_TIME], this_summary['last_mean'])
                            steady_time_var = colour_tex_cell(diff[vm][bench][STEADY_STATE_TIME_VAR], this_summary['steady_time_var'])
                        else:
                            classification = this_summary['style']
                            last_cpt = this_summary['last_cpt']
                            steady_iter_var = this_summary['steady_iter_var']
                            time_steady = this_summary['time_to_steady_state']
                            last_mean = this_summary['last_mean']
                            steady_time_var = this_summary['steady_time_var']
                        classification = '\\multicolumn{1}{l}{%s}' % classification
                        if classification == STYLE_SYMBOLS['flat']:
                            last_cpt = BLANK_CELL
                            time_steady = BLANK_CELL
                    if last_cpt == '':
                        last_cpt = BLANK_CELL
                    if time_steady == '':
                        time_steady = BLANK_CELL
                    if last_mean == '':
                        last_mean = BLANK_CELL

                    if bench_idx == 0:
                        if num_benchmarks == 10:
                            fudge = 4
                        elif num_benchmarks == 12:
                            fudge = 5
                        else:
                            fudge = 0
                        vm_cell = '\\multirow{%s}{*}{\\rotatebox[origin=c]{90}{%s}}' \
                            % (num_benchmarks + fudge, vm)
                    else:
                        vm_cell = ''
                    row_add = [BLANK_CELL, vm_cell, classification, last_cpt,
                               steady_iter_var, time_steady, last_mean, steady_time_var]
                    if not row:  # First bench in this row, needs the vm column.
                        if vm in diff and bench in diff[vm]:
                            bname = colour_tex_cell(diff[vm][bench][INTERSECTION], bench)
                        else:
                            bname = bench
                        row.insert(0, escape(bname))
                    row.extend(row_add)
                    vm_idx += 1
                fp.write('&'.join(row))
                # Only -ve space row if not next to a midrule
                if not longtable and bench_idx < num_vms - 1:
                    fp.write('\\\\[-3pt] \n')
                else:
                    fp.write('\\\\ \n')
                bench_idx += 1
            if split_row_idx < vms_per_split - 1:
                if longtable:
                    fp.write('\\hline\n')
                else:
                    fp.write('\\midrule\n')
            split_row_idx += 1
        if longtable:
            fp.write(end_longtable())
        else:
            fp.write(end_table())
        if with_preamble:
            if not longtable:
                fp.write('\\end{adjustbox}\n')
                fp.write('\\end{table*}\n')
                fp.write('\\end{landscape}\n')
            fp.write(end_document())


def create_cli_parser():
    """Create a parser to deal with command line switches."""

    description = DESCRIPTION(os.path.basename(__file__))
    parser = argparse.ArgumentParser(description=description,
                                     formatter_class=argparse.RawTextHelpFormatter)
    parser.add_argument('-j', '--json', action='store', default='diff_summary.json',
                        type=str, help='JSON file in which to write diff summary.')
    parser.add_argument('-n', '--num-splits', action='store', default=1,
                        type=int, help='Number of horizontal splits (LaTeX only).')
    parser.add_argument('--without-preamble', action='store_true',
                        dest='without_preamble', default=False,
                        help='Write out only a LaTeX table, for inclusion in a\nlarger document.')
    parser.add_argument('--vm', action='append', nargs=2, dest='vm', default=[],
                         help='Compare one VM against another. \nRequires two '
                              'VM names as arguments. By default, the\ndiffer '
                              'will compare all benchmarks / VMs which appear\n'
                              'in both input files. Users should be aware that this\n'
                              'option produces a summary file with some special JSON\n'
                              'keys. This means that summary files (usually\ndiff_summary.json) '
                              'usually not be suitable for\ngenerating generic diff tables, or '
                              'tables with different\n--vm options. In this case, users should '
                              'regenerate the\nsummary file with the original data and the -r option.')
    outputs = parser.add_mutually_exclusive_group(required=True)
    outputs.add_argument('--tex', action='store', type=str,
                         help='LaTeX file in which to write diff summary.')
    outputs.add_argument('--html', action='store', type=str,
                         help='HTML file in which to write diff summary.')
    inputs = parser.add_mutually_exclusive_group(required=True)
    inputs.add_argument('-s', '--input-summary', action='store', default=None,
                        type=str, help='Read summary data from JSON file rather than '
                                       'generating\nfrom two original results files.')
    inputs.add_argument('-r', '--input-results', nargs=2, action='append', default=[], type=str,
                        help='Exactly two Krun result files (with outliers and\nchangepoints).')
    return parser


if __name__ == '__main__':
    parser = create_cli_parser()
    options = parser.parse_args()
    diff_summary = None
    if options.html and options.without_preamble:
        print('--without-preamble only makes sense with LaTeX output. Ignoring.')
    if options.input_summary is None:
        if '_outliers' not in options.input_results[0][0]:
            fatal('Please run mark_outliers_in_json on file %s before diffing.' %
                  options.input_results[0][0])
        if '_outliers' not in options.input_results[0][1]:
            fatal('Please run mark_outliers_in_json on file %s before diffing.' %
                  options.input_results[0][1])
        if '_changepoints' not in options.input_results[0][0]:
            fatal('Please run mark_changepoints_in_json on file %s before diffing.' %
                  options.input_results[0][0])
        if '_changepoints' not in options.input_results[0][1]:
            fatal('Please run mark_changepoints_in_json on file %s before diffing.' %
                  options.input_results[0][1])
        if options.vm:
            diff_summary = diff(options.input_results[0][0], options.input_results[0][1],
                                options.json, diff_vms=options.vm[0])
        else:
            diff_summary = diff(options.input_results[0][0], options.input_results[0][1],
                                options.json, diff_vms=[])
    else:
        with open(options.input_summary, 'r') as fd:
            diff_summary = json.load(fd)
        if diff_summary is None:
            fatal('Could not open %s.' % options.input_summary)
    classifier = diff_summary[CLASSIFIER]
    if options.html:
        print('Writing data to: %s' % options.html)
        write_html_table(diff_summary[AFTER], options.html, diff=diff_summary[DIFF],
                         skipped=diff_summary[SKIPPED], previous=diff_summary[BEFORE])
    if options.tex:
        machine, bmarks, latex_summary = convert_to_latex(diff_summary[AFTER], classifier['delta'],
                                                          classifier['steady'], diff=diff_summary[DIFF],
                                                          previous=diff_summary[BEFORE])
        print('Writing data to: %s' % options.tex)
        if options.vm:
            write_latex_table(machine, bmarks, latex_summary, diff_summary[DIFF],
                              diff_summary[SKIPPED], options.tex, options.num_splits,
                              with_preamble=(not options.without_preamble),
                              longtable=True, diff_vms=options.vm[0])
        else:
            write_latex_table(machine, bmarks, latex_summary, diff_summary[DIFF],
                              diff_summary[SKIPPED], options.tex, options.num_splits,
                              with_preamble=(not options.without_preamble), longtable=True)

