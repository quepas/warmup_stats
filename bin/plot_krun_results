#!/usr/bin/env python3
#
# Copyright (c) 2017 King's College London
# created by the Software Development Team <http://soft-dev.org/>
#
# The Universal Permissive License (UPL), Version 1.0
#
# Subject to the condition set forth below, permission is hereby granted to any
# person obtaining a copy of this software, associated documentation and/or
# data (collectively the "Software"), free of charge and under any and all
# copyright rights in the Software, and any and all patent rights owned or
# freely licensable by each licensor hereunder covering either (i) the
# unmodified Software as contributed to or provided by such licensor, or (ii)
# the Larger Works (as defined below), to deal in both
#
# (a) the Software, and
# (b) any piece of software and/or hardware listed in the lrgrwrks.txt file if
# one is included with the Software (each a "Larger Work" to which the Software
# is contributed by such licensors),
#
# without restriction, including without limitation the rights to copy, create
# derivative works of, display, perform, and distribute the Software and make,
# use, sell, offer for sale, import, export, have made, and have sold the
# Software and the Larger Work(s), and to sublicense the foregoing rights on
# either these or other terms.
#
# This license is subject to the following condition: The above copyright
# notice and either this complete permission notice or at a minimum a reference
# to the UPL must be included in all copies or substantial portions of the
# Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

"""
Plot data from Krun results file(s).
"""

import argparse
import datetime
import math
import matplotlib
matplotlib.use('Agg')
import numpy
import numpy.random
import os
import os.path
import sys

from matplotlib import gridspec, pyplot
from matplotlib.collections import LineCollection

sys.path.insert(0, os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from warmup.krun_results import pretty_print_machine, read_krun_results_file
from warmup.outliers import get_window
from warmup.plotting import add_inset_to_axis, add_margin_to_axes
from warmup.plotting import collide_rect, compute_grid_offsets, format_yticks_scientific
from warmup.plotting import get_unified_yrange, style_axis, STYLE_DICT, wrap_ylabel
from warmup.plotting import zoom_y_min, zoom_y_max
from warmup.vm_instruments import INSTRUMENTATION_PARSERS

pyplot.figure(tight_layout=True)

# Set matplotlib styles, similar to Seaborn 'whitegrid'.
for style in STYLE_DICT:
    matplotlib.rcParams[style] = STYLE_DICT[style]

# Configures border and spacing of subplots.
# Here we just make it more space efficient for the paper
SUBPLOT_PARAMS = {
    'hspace': 0.35,
    'bottom': 0.07,
    'left': 0.07,
    'right': 0.93,
    'top': 0.90,
    'wspace': 0.30,
}

# Display names that can't be formatted with .title()
BENCHMARKS = {
    'binarytrees':      'Binary Trees',
    'spectralnorm':     'Spectral Norm',
    'fannkuch_redux':   'Fannkuch Redux',
    'nbody':            'N-Body',
}

VMS = {
    'JRubyTruffle':     'TruffleRuby',
}

GRID_MINOR_X_DIVS = 20
GRID_MAJOR_X_DIVS = 10

GRID_MINOR_Y_DIVS = 12
GRID_MAJOR_Y_DIVS = 6

GRID_MINOR_Y_DIVS_SMALLER_PLOTS = 4
GRID_MAJOR_Y_DIVS_SMALLER_PLOTS = 2

LINE_COLOUR = '0.6'
LABEL_COLOUR = 'k'
STEADY_COLOUR = 'k'
FILL_ALPHA = 0.2

# line widths, measured in pts
LINE_WIDTH = 0.4

OUTLIER_MARKER = 'o'
OUTLIER_COLOR = 'r'
OUTLIER_SIZE = 50
UNIQUE_COLOR = 'g'
UNIQUE_MARKER = 'o'
UNIQUE_SIZE = 50
COMMON_COLOR = 'r'
COMMON_MARKER = '*'
COMMON_SIZE = 50
CYCLES_COLOR = 'black'
PERF_COLOR = 'black'
INSTR_COLOR = 'black'

CHANGEPOINT_LINE_STYLE = '--'
CHANGEPOINT_LINE_COLOR = 'r'
CHANGEPOINT_LINE_WIDTH = 0.8

ZORDER_GRID = 1
ZORDER_DATA = 5
ZORDER_CHANGEPOINTS = 10
ZORDER_MARKERS = 20
ZORDER_INSET_BACKGROUND = 30

# Gap between right-hand y-axes.
TWINX_OFFSET = 1.1

# Padding for unified y-limits.
Y_LIM_PADDING = 0.002

# Use a fixed y-range for the aperf/mperf data.
PERF_YRANGE = (0.0, 1.2)
PERF_YTICK_FORMAT = '%.1f'

# Inset placement (left, bottom, width, height) relative to subplot axis.
INSET_DEFAULT_WIDTH = 0.3
INSET_DEFAULT_HEIGHT = 0.2
INSET_PADDING = 0.06
INSET_TICK_SPACE = 0.07
INSET_DIST_DELTA = 0.01
INSET_MIN_ITERS = 50
INSET_MAX_ITERS = 150
INSET_TICK_FONTSIZE = 8

# Zoomed in plot of measurement data.
ZOOM_PROPORTION = 0.02
ZOOM_EXTRA_Y_LIM_PADDING = 0.1
ZOOM_OUTLIER_SIZE = 8

# Default (PDF) font sizes
TICK_FONTSIZE = 8
TITLE_FONT_SIZE = 10
AXIS_FONTSIZE = 8
YAXIS_FONTSIZE = 8
YTICK_FORMAT = '%.4f'

MAX_SUBPLOTS_PER_ROW = 2
EXPORT_SIZE_INCHES = [12, 10]
DPI = 300


def get_instr_data(key, machine, instr_dir, pexec_idxs):
    """Get the instrumentation data summary for the specified process execution
    indexes"""

    bench, vm, variant = key.split(":")
    ret = []
    if vm in INSTRUMENTATION_PARSERS:
        for pexec_idx in pexec_idxs:
            file_ = os.path.join(instr_dir, "%s__%s__%s__%s.json.bz2" %
                                 (bench, vm, variant, pexec_idx))
            print('Loading: %s' % file_)
            try:
                js = read_krun_results_file(file_)
            except IOError:
                print('WARNING: Missing instrumentation data for: %s:%s:%s' % \
                      (machine, key, pexec_idx))
                ret.append(None)  # missing instr data
                continue
            parser = INSTRUMENTATION_PARSERS[vm](js)
            ret.append(parser.chart_data)
            del js  # This can be huge, so eagerly GC it
        return ret


def main(is_interactive, data_dcts, plot_titles, window_size, outfile,
         xlimits, with_outliers, unique_outliers, changepoint_means,
         inset=False, zoom=True, one_page=False,
         core_cycles=(0,1,2,3), cycles_ylimits=None,
         inset_xlimits=None):
    """Determine which plots to put on each page of output.
    Plot all data.
    """

    pdf = None  # PDF output (for non-interactive mode).

    if not is_interactive:
        pdf = PdfPages(outfile)
        set_pdf_metadata(pdf)

    # Run sequences, outliers and subplot titles for each page we need to plot.
    pages, all_subplot_titles = list(), list()
    cycles_pages = list()
    instr_pages = list()
    all_outliers, all_common, all_unique = list(), list(), list()
    all_changepoints, all_changepoint_means, all_changepoint_vars = list(), list(), list()
    all_classifications = list()
    classifier = data_dcts['classifier']

    # By default, each benchmark from each machine is placed on a separate
    # page. If the --one-page switch has been passed in, then we place
    # all plots on a single page. In which case, we need to construct
    # a flat list of all run sequences.
    if one_page:
        page, subplot_titles = list(), list()
        cycles_page = list()
        instr_page = list()
        page_common, page_unique, page_outlier = list(), list(), list()
        page_changepoints, page_changepoint_means, page_changepoint_vars = list(), list(), list()
        page_classifications = list()
        for key in sorted(data_dcts['data']):
            for machine in sorted(data_dcts['data'][key]):
                for index, run_seq in enumerate(data_dcts['data'][key][machine]):
                    page.append(run_seq)
                    if data_dcts['cycles_counts'][key][machine]:
                        cycles_page.append(data_dcts['cycles_counts'][key][machine][index])
                    if data_dcts['instr_data'][key][machine]:
                        instr_page.append(data_dcts['instr_data'][key][machine][index])
                    subplot_titles.append(plot_titles[key][machine][index])
                    # Collect outliers, if the user wishes to annotate them on
                    # the plots. The draw_page() and draw_subplot() functions
                    # expect either lists of outliers or None (if outliers are
                    # not to be annotated).
                    if with_outliers:
                        page_outlier.append(data_dcts['all_outliers'][key][machine][index])
                    else:
                        page_outlier.append(None)
                    if unique_outliers:
                        page_common.append(data_dcts['common_outliers'][key][machine][index])
                        page_unique.append(data_dcts['unique_outliers'][key][machine][index])
                    else:
                        page_common.append(None)
                        page_unique.append(None)
                    if changepoint_means:
                        page_changepoints.append(data_dcts['changepoints'][key][machine][index])
                        page_changepoint_means.append(data_dcts['changepoint_means'][key][machine][index])
                        page_changepoint_vars.append(data_dcts['changepoint_vars'][key][machine][index])
                        page_classifications.append(data_dcts['classifications'][key][machine][index])
                    else:
                        page_changepoints.append(None)
                        page_changepoint_means.append(None)
                        page_changepoint_vars.append(None)
                        page_classifications.append(None)
        pages.append(page)
        cycles_pages.append(cycles_page)
        instr_pages.append(instr_page)
        all_subplot_titles.append(subplot_titles)
        all_outliers.append(page_outlier)
        all_common.append(page_common)
        all_unique.append(page_unique)
        all_changepoints.append(page_changepoints)
        all_changepoint_means.append(page_changepoint_means)
        all_changepoint_vars.append(page_changepoint_vars)
        all_classifications.append(page_classifications)
    else:  # Create multiple pages.
        for key in sorted(data_dcts['data']):
            for machine in sorted(data_dcts['data'][key]):
                pages.append(data_dcts['data'][key][machine])
                cycles_pages.append(data_dcts['cycles_counts'][key][machine])
                instr_pages.append(data_dcts['instr_data'][key][machine])
                all_subplot_titles.append(plot_titles[key][machine])
                if with_outliers:
                    all_outliers.append(data_dcts['all_outliers'][key][machine])
                else:
                    all_outliers.append(None)
                if unique_outliers:
                    all_common.append(data_dcts['common_outliers'][key][machine])
                    all_unique.append(data_dcts['unique_outliers'][key][machine])
                else:
                    all_common.append(None)
                    all_unique.append(None)
                if changepoint_means:
                    all_changepoints.append(data_dcts['changepoints'][key][machine])
                    all_changepoint_means.append(data_dcts['changepoint_means'][key][machine])
                    all_changepoint_vars.append(data_dcts['changepoint_vars'][key][machine])
                    all_classifications.append(data_dcts['classifications'][key][machine])
                else:
                    all_changepoints.append(None)
                    all_changepoint_means.append(None)
                    all_changepoint_vars.append(None)
                    all_classifications.append(None)

    # Draw each page and display (interactive mode) or save to disk.
    try:
        for index, page in enumerate(pages):
            bmark, vm, mc = all_subplot_titles[index][0].split(', ')[:3]
            print('Plotting %s: %s (%s) on page %02d of %02d.' % \
                  (mc, bmark, vm, index + 1, len(pages)))

            # Strip out indices where the benchmark crashed.
            def only_uncrashed(data):
                if data is None:
                    return None
                ret = list()
                for i in range(len(page)):
                    if page[i]:
                        try:
                            ret.append(data[i])
                        except IndexError:
                            # Absent data
                            ret.append([])
                    else:
                        if data == page:  # Stops repeated printing of warning.
                            print('WARNING: requested pexec crashed: '
                                  '%s, %s, %s, %s' % (mc, bmark, vm, i))
                return ret

            wct_page = only_uncrashed(page)
            cycles_page = only_uncrashed(cycles_pages[index])
            instr_page = only_uncrashed(instr_pages[index])
            subplot_titles = only_uncrashed(all_subplot_titles[index])
            outliers = only_uncrashed(all_outliers[index])
            common = only_uncrashed(all_common[index])
            unique = only_uncrashed(all_unique[index])
            changepoints = only_uncrashed(all_changepoints[index])
            changepoint_means = only_uncrashed(all_changepoint_means[index])
            changepoint_vars = only_uncrashed(all_changepoint_vars[index])
            classifications = only_uncrashed(all_classifications[index])

            fig = draw_page(is_interactive, wct_page, cycles_page,
                                         instr_page, subplot_titles,
                                         window_size, xlimits, outliers,
                                         unique, common, changepoints,
                                         changepoint_means, changepoint_vars,
                                         classifications, classifier,
                                         inset, zoom,
                                         core_cycles, cycles_ylimits, inset_xlimits)
            if fig is not None:
                if not is_interactive:
                    pdf.savefig(fig, dpi=fig.dpi, orientation='landscape',
                                bbox_inches='tight')
                    pyplot.close()
    except KeyboardInterrupt:
        pass  # Avoid printing a traceback.
    finally:
        if not is_interactive:
            pdf.close()
            print('Saved: %s' % outfile)


class ProcessExecChart(object):
    """This class represents a plot, or stack of plots for a single process execution.
    """

    def __init__(self, grid_cell, data, cycles_data, instr_data, instr_y_ranges,
                 title, x_bounds, y_range, y_range_zoom, window_size, outliers,
                 unique, common, changepoints, changepoint_means, changepoint_vars,
                 classification, classifier, core_cycles,
                 cycles_ylimits, inset_xbounds):
        self.grid_cell = grid_cell
        self.title = title
        self.window_size = window_size
        self.x_bounds = x_bounds
        self.iterations = numpy.array(range(x_bounds[0], x_bounds[1]))
        self.y_range = y_range
        self.y_range_zoom = y_range_zoom
        if outliers:
            self.outliers = self._get_scatter_points_within_bounds(outliers, self.x_bounds)
        else:
            self.outliers = None
        if unique:
            self.unique = self._get_scatter_points_within_bounds(unique, self.x_bounds)
        else:
            self.unique = None
        if common:
            self.common = self._get_scatter_points_within_bounds(common, self.x_bounds)
        else:
            self.common = None
        # Marshal changepoint data.
        self.changepoints = list()
        if changepoint_means:
            self.changepoint_means = [changepoint_means[0]]
            self.changepoint_vars = [changepoint_vars[0]]
        else:
            self.changepoint_means = list()
            self.changepoint_vars = list()
        self.classification = classification
        self.steady_equivalents = list()  # Used by self._plot_steady_equivalent
        if changepoint_means:
            self.delta = classifier['delta']
            if changepoints:
                self.last_changepoint = changepoints[-1]
            else:
                self.last_changepoint = None
            self.last_segment_mean = changepoint_means[-1]
            self.last_segment_var = changepoint_vars[-1]
            self.segment_means = list()  # ((x0, y0), (x1, y1)) pairs.
            for index, changepoint in enumerate(changepoints):
                if changepoint >= self.x_bounds[0] and changepoint <= x_bounds[1]:
                    self.changepoints.append(changepoints[index])
                    self.changepoint_means.append(changepoint_means[index + 1])
                    self.changepoint_vars.append(changepoint_vars[index + 1])
            assert len(self.changepoints) == len(self.changepoint_means) - 1
            assert len(self.changepoints) == len(self.changepoint_vars) - 1
            for index, x_location in enumerate(self.changepoints):
                if index == 0:
                    self.segment_means.append([(self.x_bounds[0], self.changepoint_means[0]),
                                 (x_location, self.changepoint_means[0])])
                else:
                    self.segment_means.append([(self.changepoints[index - 1], self.changepoint_means[index]),
                                 (x_location, self.changepoint_means[index])])
            if len(self.changepoints) == 0:
                self.segment_means.append([(self.x_bounds[0], self.changepoint_means[-1]),
                             (self.x_bounds[1], self.changepoint_means[-1])])
            else:
                self.segment_means.append([(self.changepoints[-1], self.changepoint_means[-1]),
                             (self.x_bounds[1], self.changepoint_means[-1])])
            assert len(self.segment_means) == len(self.changepoint_means)
        self.core_cycles = core_cycles
        self.cycles_ylimits = cycles_ylimits
        self.instr_y_ranges = instr_y_ranges
        # Marshal measurement data into numpy arrays.
        self.wallclock_data = numpy.array(data[x_bounds[0]:x_bounds[1]])
        self.cycles_data = list()
        if cycles_data:
            if cycles_ylimits:
                self.cycles_min, self.cycles_max = cycles_ylimits
            else:
                if self.core_cycles is None:
                    self.core_cycles = range(len(cycles_data))
                self.cycles_min, self.cycles_max = float('inf'), float('-inf')
                for core in self.core_cycles:
                    self.cycles_min = min(min(cycles_data[core][x_bounds[0]:x_bounds[1]]), self.cycles_min)
                    self.cycles_max = max(max(cycles_data[core][x_bounds[0]:x_bounds[1]]), self.cycles_max)
            if self.cycles_min < 0.0:
                self.cycles_min = 0.0
            for core in self.core_cycles:  # For each core specified on command line.
                self.cycles_data.append(numpy.array(cycles_data[core][x_bounds[0]:x_bounds[1]]))
        self.instr_data = list()
        if instr_data:
            self.instr_data = instr_data
        # Shared information for grid styles. We treat the x-ticks as a special
        # case, we want the xticklabels to be a closed interval, e.g. if the
        # indices of self.wallclock_times runs from 0-99, we want the xticklabels
        # to run from 0-99 (not 0-100), then later we increment them by one, so
        # they are not array-indexed (i.e. the final labels will run from 1-100).
        self.major_xticks = compute_grid_offsets(self.x_bounds[0], self.x_bounds[1],
                                                 GRID_MAJOR_X_DIVS, with_max=True)
        self.minor_xticks = list()  # Should appear half-way between major xticks.
        for index, value in enumerate(self.major_xticks[:-1]):
            self.minor_xticks.append(value + ((self.major_xticks[index + 1] - value) / 2))
        # Calculate the number of plots needed.
        self.n_rows = 1
        if cycles_data:
            self.n_rows += len(self.core_cycles)
        if instr_data:
            self.n_rows += len(self.instr_data)
        if y_range_zoom[0] != None:
            self.n_rows += 1
        if self.n_rows == 1:  # Only create another gridspec if we have to.
            self.inner_grid = None
        else:
            # Spring constraints for grid cell heights
            heights = [.25] * (self.n_rows - 1)
            heights.append(1)
            self.inner_grid = gridspec.GridSpecFromSubplotSpec(self.n_rows, 1,
                              self.grid_cell, height_ratios=heights, hspace=0.2)
        # Scaffold axes.
        self.wallclock_axis = None
        self.cycles_axes = None
        self.instr_axes = None
        self.zoomed_axis = None
        self.inset = None
        self.inset_xlimit = inset_xbounds

    def plot_stack(self):
        self.plot_wallclock_times()
        self.row = 0  # Number of smaller plots. Some methods add several charts.
        if self.cycles_data:
            self.plot_cycles_data()
        if self.instr_data:
            self.plot_instr_data()
        if self.y_range_zoom[0] != None:
            self.plot_zoomed_data()
        self._increment_xticklabels(self.wallclock_axis)
        add_margin_to_axes(self.wallclock_axis, x=0.02, y=0.02)

    def _increment_xticklabels(self, axis):
        """Add one to each xticklabel, so that iterations are not array-indexed."""
        pyplot.draw()  # Force xticklabels to be drawn.
        labels = [label.get_text() for label in axis.get_xticklabels()]
        try:
            new_labels = [int(label) + 1 if label else '' for label in labels]
        except ValueError:  # Labels are floats.
            new_labels = [float(label) + 1.0 if label else '' for label in labels]
        axis.set_xticklabels(new_labels)

    def style_axis(self, axis, y_range, x_label, y_label, color=LINE_COLOUR, smaller_plot=True):
        axis.set_xlim(self.x_bounds[0], self.x_bounds[1] - 1)
        axis.set_ylim(y_range)
        if smaller_plot:
            major_y_divs = GRID_MAJOR_Y_DIVS_SMALLER_PLOTS
            minor_y_divs = GRID_MINOR_Y_DIVS_SMALLER_PLOTS
        else:
            major_y_divs = GRID_MAJOR_Y_DIVS
            minor_y_divs = GRID_MINOR_Y_DIVS
        major_yticks = compute_grid_offsets(y_range[0], y_range[1], major_y_divs)
        minor_yticks = compute_grid_offsets(y_range[0], y_range[1], minor_y_divs)
        style_axis(axis, self.major_xticks, self.minor_xticks, major_yticks,
                   minor_yticks, TICK_FONTSIZE)
        format_yticks_scientific(axis)
        if x_label:
            axis.set_xlabel(x_label, fontsize=AXIS_FONTSIZE, color=LABEL_COLOUR)
        if smaller_plot:
            pyplot.setp(axis.get_xticklabels(), visible=False)
            y_axis = axis.get_yaxis()
            y_axis.set_tick_params(labelsize=TICK_FONTSIZE, colors=LABEL_COLOUR, zorder=ZORDER_GRID)
        axis.set_ylabel(y_label, fontsize=YAXIS_FONTSIZE, color=LABEL_COLOUR)
        axis.yaxis.set_label_position('right')

    def _get_scatter_points_within_bounds(self, scatter, x_bounds):
        """Given a set of x-locations to be plotted as a scatter plot, move the
        marker locations to within x_bounds. This is needed when we have a set
        of outliers to plot, but we only want to view a small section of the
        whole chart.
        """
        x_vals = [val for val in scatter if val >= x_bounds[0] and val < x_bounds[1]]
        y_vals = [val - x_bounds[0] for val in scatter if val >= x_bounds[0] and val < x_bounds[1]]
        return x_vals, y_vals

    def plot_wallclock_times(self):
        if self.n_rows == 1:
            self.wallclock_axis = pyplot.subplot(self.grid_cell)
        else:
            self.wallclock_axis = pyplot.subplot(self.inner_grid[self.n_rows - 1, 0])
        self.wallclock_axis.plot(self.iterations, self.wallclock_data, label='Measurement',
                        color=LINE_COLOUR, zorder=ZORDER_DATA, linewidth=LINE_WIDTH)
        self.wallclock_axis.autoscale(enable=False, axis='both')
        self._plot_changepoints(self.wallclock_axis)
        self._plot_outliers(self.wallclock_axis, large_markers=True)
        self._plot_steady_equivalent(self.wallclock_axis)
        self.style_axis(self.wallclock_axis, self.y_range, 'In-process iteration',
                        'Time (secs)', smaller_plot=False)
        # Plot title goes above the top subplot (only once).
        if not self.cycles_data and not self.instr_data and self.y_range_zoom[0] == None:
            self.wallclock_axis.set_title(self.title, fontsize=TITLE_FONT_SIZE)

    def _plot_outliers(self, axis, large_markers=True):
        if large_markers:
            size = OUTLIER_SIZE
            outlier_marker = OUTLIER_MARKER
            unique_marker = UNIQUE_MARKER
            common_marker = COMMON_MARKER
        else:
            size = ZOOM_OUTLIER_SIZE
            outlier_marker = OUTLIER_MARKER
            unique_marker = OUTLIER_MARKER
            common_marker = OUTLIER_MARKER
        if self.outliers:
            axis.scatter(self.outliers[0], self.wallclock_data[self.outliers[1]],
                         color=OUTLIER_COLOR, marker=outlier_marker, s=size,
                         label='Outliers', zorder=ZORDER_MARKERS)
        if self.unique:
            pc_unique = float(len(self.unique[1]) - self.window_size) / float(len(self.wallclock_data)) * 100.0
            axis.scatter(self.unique[0], self.wallclock_data[self.unique[1]],
                         color=UNIQUE_COLOR, marker=unique_marker, s=size,
                         label=('Unique outliers (${%.2f}\\%%$)' % pc_unique),
                         zorder=ZORDER_MARKERS)
        if self.common:
            pc_common = float(len(self.common[1])) / float(len(self.wallclock_data)) * 100.0
            axis.scatter(self.common[0], self.wallclock_data[self.common[1]],
                         color=COMMON_COLOR, marker=common_marker, s=size,
                         label=('Common outliers (${%.2f}\\%%$)' % pc_common),
                         zorder=ZORDER_MARKERS)

    def _plot_steady_equivalent(self, axis):
        """Plot steady state segment and 'equivalents' in a different colour."""

        if not self.changepoint_means or self.classification == 'flat':
            axis.plot(self.iterations, self.wallclock_data,
                      color=STEADY_COLOUR, zorder=ZORDER_DATA + 1, linewidth=LINE_WIDTH)
            return  # Whole plot is black.
        elif self.classification == 'no steady state':
            return  # Whole plot is grey.
        self.steady_equivalents = list()  # List of (start, end) pairs.
        lower_bound = min(self.last_segment_mean - self.last_segment_var,
                          self.last_segment_mean - self.delta)
        upper_bound = max(self.last_segment_mean + self.last_segment_var,
                          self.last_segment_mean + self.delta)
        if len(self.changepoints) > 0:
            # Plot earlier, equivalent segments.
            for index in range(len(self.changepoint_means) - 2, -1, -1):
                current_segment_mean = self.changepoint_means[index]
                current_segment_var = self.changepoint_vars[index]
                if (current_segment_mean + current_segment_var >= lower_bound and
                        current_segment_mean - current_segment_var <= upper_bound):
                    if index == 0:  # (start, end) are array indices.
                        start = 0
                        end = self.changepoints[index] - self.x_bounds[0]
                    else:
                        start = self.changepoints[index - 1] - self.x_bounds[0]
                        end = self.changepoints[index] - self.x_bounds[0]
                    self.steady_equivalents.append((start, end))
                    axis.plot(self.iterations[start:end], self.wallclock_data[start:end],
                              color=STEADY_COLOUR, zorder=ZORDER_DATA + 1, linewidth=LINE_WIDTH)
        elif len(self.changepoints) == 0 and self.x_bounds[0] != 0:
            # No changepoints, but user passed in --xbounds, so we need to check
            # whether the remaining segment mean is equivalent to the steady state.
            if (self.segment_means[0] + self.segment_vars[0] >= lower_bound and
                    self.segment_means[0] - self.segment_var[0] <= upper_bound):
                self.steady_equivalents.append((self.x_bounds[0], self.x_bounds[1]))
                axis.plot(self.iterations,
                          self.wallclock_data[self.x_bounds[0]:self.x_bounds[0]],
                          color=STEADY_COLOUR, zorder=ZORDER_DATA + 1, linewidth=LINE_WIDTH)
        # Highlight steady-state segment.
        if self.last_changepoint < self.x_bounds[1]:
            self.steady_equivalents.append((self.last_changepoint, self.x_bounds[1]))
            axis.plot(self.iterations[self.last_changepoint:],
                      self.wallclock_data[self.last_changepoint - self.x_bounds[0]:],
                      color=STEADY_COLOUR, zorder=ZORDER_DATA + 1, linewidth=LINE_WIDTH)

    def _plot_changepoints(self, axis):
        if self.changepoint_means:
            for index, x_location in enumerate(self.changepoints):
                axis.axvline(x_location, linestyle=CHANGEPOINT_LINE_STYLE,
                             zorder=ZORDER_CHANGEPOINTS, color=CHANGEPOINT_LINE_COLOR,
                             linewidth=CHANGEPOINT_LINE_WIDTH)
            lines = LineCollection(self.segment_means, color=CHANGEPOINT_LINE_COLOR,
                                   zorder=ZORDER_CHANGEPOINTS, linestyle=CHANGEPOINT_LINE_STYLE,
                                   linewidths=[CHANGEPOINT_LINE_WIDTH
                                   for _ in range(len(self.segment_means))])
            axis.add_collection(lines)

    def plot_cycles_data(self):
        """Add core cycles data on another set of charts."""
        self.cycles_axes = [None] * len(self.cycles_data)
        for core in range(len(self.cycles_data)):
            self.cycles_axes[core] = pyplot.subplot(self.inner_grid[self.row, 0],
                                                    sharex=self.wallclock_axis)
            self.cycles_axes[core].plot(self.iterations, self.cycles_data[core],
                      color=CYCLES_COLOR, label=('Core %d cycles' % self.core_cycles[core]),
                      linewidth=LINE_WIDTH, zorder=ZORDER_DATA)
            self.style_axis(self.cycles_axes[core], (self.cycles_min, self.cycles_max),
                            None, 'CC #%d' % self.core_cycles[core], color=CYCLES_COLOR)
            if core == 0:
                # Plot title goes above the top subplot (only once).
                self.cycles_axes[core].set_title(self.title, fontsize=TITLE_FONT_SIZE)
            self.row += 1

    def plot_instr_data(self):
        """Add any VM instrumentation data on another set of charts."""
        self.instr_axes = [None] * len(self.instr_data)
        for index, idata in enumerate(self.instr_data):
            self.instr_axes[index] = pyplot.subplot(self.inner_grid[self.row, 0],
                                                    sharex=self.wallclock_axis)
            self.instr_axes[index].plot(self.iterations, idata.data[self.x_bounds[0]:self.x_bounds[1]],
                        color=INSTR_COLOR, linewidth=LINE_WIDTH, zorder=ZORDER_DATA)
            self.instr_axes[index].set_ylim(self.instr_y_ranges[index])
            self.style_axis(self.instr_axes[index], self.instr_y_ranges[index],
                            None, wrap_ylabel(idata.title), color=INSTR_COLOR)
            if index == 0:
                # Plot title goes above the top subplot (only once).
                if not self.cycles_data:
                    self.instr_axes[index].set_title(self.title, fontsize=TITLE_FONT_SIZE)
            self.row += 1

    def plot_zoomed_data(self):
        """Add zoomed in chart with wallclock data."""
        self.zoomed_axis = pyplot.subplot(self.inner_grid[self.row, 0], sharex=self.wallclock_axis)
        self.zoomed_axis.autoscale(enable=False, axis='both') # Set x/y-limits manually.
        pyplot.setp(self.zoomed_axis.get_xticklabels(), visible=False)
        self.zoomed_axis.plot(self.iterations, self.wallclock_data, label='Measurement',
                              color=LINE_COLOUR, zorder=ZORDER_DATA, linewidth=LINE_WIDTH)
        self.style_axis(self.zoomed_axis, self.y_range_zoom, None, 'Time (secs)')
        add_margin_to_axes(self.zoomed_axis, x=0.0, y=ZOOM_EXTRA_Y_LIM_PADDING)
        self._plot_changepoints(self.zoomed_axis)
        self._plot_outliers(self.zoomed_axis, large_markers=False)
        # Plot title goes above the top subplot (only once).
        if not self.cycles_data and not self.instr_data:
            self.zoomed_axis.set_title(self.title, fontsize=TITLE_FONT_SIZE)
        self._plot_steady_equivalent(self.zoomed_axis)
        self.row += 1

    def plot_inset(self, axis, fig):
        """Draw an inset containing the early iterations of the wallclock data.
        This MUST be done after adjusting subplots, so that we can calculate
        the correct bounding box for the inset."""
        # Find a rectangle that will not collide with data. Start top-right
        # and work left, then try bottom-right and work left. INSET_PADDING
        # is added because we add a little padding to the whole chart so that
        # the first and last data points are visible. We add INSET_TICK_SPACE
        # to account for the x and y tick labels which matplotlib does not
        # include in fig.add_axes.
        inset_collides = False
        left_offsets = [x / 10.0 for x in range(6)] * 2
        bottom_values = (([1.0 - INSET_DEFAULT_HEIGHT - INSET_PADDING - INSET_TICK_SPACE] * (len(left_offsets) // 2))
                         + ([INSET_PADDING * 2 + INSET_TICK_SPACE] * (len(left_offsets) // 2)))
        best_distance = -1.0
        best_rect = None
        for left_offset, bottom in zip(left_offsets, bottom_values):
            rect = [1.0 - left_offset - INSET_DEFAULT_WIDTH - INSET_PADDING - INSET_TICK_SPACE,
                    bottom, INSET_DEFAULT_WIDTH + INSET_TICK_SPACE, INSET_DEFAULT_HEIGHT + INSET_TICK_SPACE]
            inset_collides, dist = collide_rect(rect[0], rect[1], rect[2], rect[3], fig, axis, self.wallclock_data, self.x_bounds)
            if not inset_collides:
                if not best_rect or round(dist - best_distance) >= INSET_DIST_DELTA:
                    best_distance = dist
                    best_rect = rect
        if best_rect:
            # Draw a white background in the inflated rectangle.
            axis.add_patch(pyplot.Rectangle(best_rect[:2], best_rect[2], best_rect[3],
                           transform=axis.transAxes, color='white',
                           zorder=ZORDER_INSET_BACKGROUND))
            # Remove tick spacing before placing inset on axis.
            inset = add_inset_to_axis(fig, axis, [best_rect[0] + INSET_TICK_SPACE,
                best_rect[1] + INSET_TICK_SPACE, best_rect[2] - INSET_TICK_SPACE - (INSET_PADDING / 2),
                best_rect[3] - INSET_TICK_SPACE - (INSET_PADDING / 2)])
            inset.grid(False)
            format_yticks_scientific(inset)
            inset.xaxis.set_tick_params(labelsize=INSET_TICK_FONTSIZE)
            inset.yaxis.set_tick_params(labelsize=INSET_TICK_FONTSIZE)
            # Which iterations go in the inset?
            # Unless specified by the user, the first 2.5%.
            if not self.inset_xlimit:
                one_pc = len(self.wallclock_data[self.x_bounds[0]:self.x_bounds[1]]) / 100.0  # 1%.
                inset_xlimit = (self.x_bounds[0], self.x_bounds[0] + int(one_pc * 2.5))
                # If all the changepoints occur at the start of the process exec,
                # include them all and a few extra executions.
                if self.changepoints:
                    if (self.changepoints[self.x_bounds[0]:self.x_bounds[1]] and
                          self.changepoints[self.x_bounds[0]:self.x_bounds[1]][-1] <= int(one_pc * 5.0)):
                        inset_xlimit = (0, (self.changepoints[self.x_bounds[0]:self.x_bounds[1]][-1]
                                            + int(one_pc * 2.0)))
                # Clamp the extent of the inset.
                if inset_xlimit[1] < INSET_MIN_ITERS:
                    inset_xlimit = (self.x_bounds[0], min(INSET_MIN_ITERS, self.x_bounds[1] - self.x_bounds[0] - 1))
                elif inset_xlimit[1] > INSET_MAX_ITERS:
                    inset_xlimit = (self.x_bounds[0], min(INSET_MAX_ITERS, self.x_bounds[1] - self.x_bounds[0] - 1))
            else:
                inset_xlimit = self.inset_xlimit

            # Plot a subset of the data on the inset. start, end are array
            # indices, to be passed to self.wallclock_data which has already
            # been sliced to match self.x_bounds.
            if not self.inset_xlimit:  # User has not specified --inset-xlimit.
                start, end = inset_xlimit[0] + self.x_bounds[0], inset_xlimit[1] + self.x_bounds[0] + 1
            else:
                start, end = inset_xlimit[0] - self.x_bounds[0], inset_xlimit[1] - self.x_bounds[0] + 1

            # Plot data before changing other artists.
            inset.plot(range(inset_xlimit[0], inset_xlimit[1] + 1), self.wallclock_data[start:end],
                       color=LINE_COLOUR, linewidth=LINE_WIDTH, zorder=ZORDER_DATA)
            for segment_start, segment_end in self.steady_equivalents:
                if segment_start <= inset_xlimit[1]:
                    rhs = min(segment_end, inset_xlimit[1])
                    inset.plot(range(segment_start, rhs),
                               self.wallclock_data[segment_start:rhs],
                               color=STEADY_COLOUR, linewidth=LINE_WIDTH, zorder=ZORDER_DATA)
            inset.set_xlim(inset_xlimit[0], inset_xlimit[1], auto=False)
            self._plot_outliers(inset, large_markers=False)
            self._plot_changepoints(inset)

            # Scale the y-axis to only the data the inset shows.
            y_min, y_max = get_unified_yrange([self.wallclock_data], start,
                                              end, padding=0.02)
            inset.set_ylim([y_min, y_max])
            inset.set_yticks([y_min, y_min + ((y_max - y_min) / 2.0), y_max])

            # As with the xticklabels on the wallclock_times plot, we want to
            # use a closed interval here, and we do NOT use array indices.
            xtick_locations = [inset_xlimit[0],
                               inset_xlimit[0] + round((inset_xlimit[1] - inset_xlimit[0]) / 2.0),
                               inset_xlimit[1] - 1]
            inset.set_xticks(xtick_locations)
            self._increment_xticklabels(inset)


def draw_page(is_interactive, executions, cycles_executions,
              instr_executions, titles, window_size, xlimits,
              outliers, unique, common, changepoints, changepoint_means,
              changepoint_vars, classifications, classifier,
              inset=False, zoom=True,
              core_cycles=(0,1,2,3), cycles_ylimits=None, inset_xlimits=None):
    """Plot a page of benchmarks.
    """

    n_execs = len(executions)
    if n_execs == 0:
        print('WARNING: empty page')
        return None

    n_rows = int(math.ceil(float(len(executions)) / MAX_SUBPLOTS_PER_ROW))
    n_cols = min(MAX_SUBPLOTS_PER_ROW, n_execs)

    print('%g plots arranged in %g rows and %g columns.'
          % (len(executions), n_rows, n_cols))

    if xlimits is None:
        xlimits_start = 0
        xlimits_stop = len(executions[0])  # Assume all execs are the same length
    else:
        xlimits_start = xlimits[0]
        xlimits_stop = xlimits[1]
        if xlimits_start < 0 or xlimits_stop > len(executions[0]):
            fatal_error('You specified %s as xlimits, but your data contains'
                        ' iterations between 0 and %d' % (xlimits, len(executions[0])))

    if inset_xlimits:
        if inset_xlimits[0] < 0 or inset_xlimits[1] > len(executions[0]):
            fatal_error('You specified %s as inset xlimits, but your data'
                        'contains iterations between 0 and %d' %
                        (inset_xlimits, len(executions[0])))
        inset_x_bounds = inset_xlimits
    else:
        inset_x_bounds = None  # Meaning, figure it out automatically

    # Find the min and max y values across all wallclock time plots for this page.
    y_min, y_max = get_unified_yrange(executions, xlimits_start, xlimits_stop, padding=Y_LIM_PADDING)

    # Find y-limits for a zoomed-in plot for each execution on this page.
    y_range_zoom = list()
    if not zoom:  # User requested no zoom plot.
        y_range_zoom = [(None, None)] * len(executions)
    else:
        for index in range(len(executions)):
            if window_size and window_size > 0:
                first_n = window_size
            else:
                first_n = int(len(executions[index]) * ZOOM_PROPORTION)
            if outliers:
                y_zoom_min = zoom_y_min(executions[index], outliers[index], start_from=first_n)
                y_zoom_max = zoom_y_max(executions[index], outliers[index], start_from=first_n)
            else:
                y_zoom_min = min(executions[index][first_n:])
                y_zoom_max = max(executions[index][first_n:])
            y_range_zoom.append((y_zoom_min, y_zoom_max))

    # Get unified y-ranges for the instrumentation data. Each VM may have more
    # more than one set of instrumentation data (e.g. GC events, JIT
    # compilation, etc.) and each VM instrument must have its own y-range.
    if instr_executions:
        instr_data_by_instrument = []
        for execution in instr_executions:
            if execution is None:
                continue  # absent instr data for this pexec
            instr_data_by_instrument = [list() for _ in execution]
            break
        for execution in instr_executions:
            if execution is None:
                for index in range(len(instr_data_by_instrument)):
                    instr_data_by_instrument[index].append(None)
            else:
                for index, instrument_data in enumerate(execution):
                    instr_data_by_instrument[index].append(instrument_data.data)
        instr_y_ranges = list()
        for chart_data in instr_data_by_instrument:
            chart_data_stripped = [x for x in chart_data if x is not None]
            instr_y_ranges.append(get_unified_yrange(chart_data_stripped, xlimits_start,
                                                     xlimits_stop, padding=Y_LIM_PADDING))
    else:
        instr_y_ranges = None

    fig = pyplot.figure()
    # Set figure size here, so coordinates are correct before we draw insets.
    if not is_interactive:
        export_size = (EXPORT_SIZE_INCHES[0] * n_cols, EXPORT_SIZE_INCHES[1] * n_rows)
        fig.set_size_inches(*export_size)

    outer_grid = gridspec.GridSpec(n_rows, n_cols)
    p_exec_charts = list()  # ProcessExecChart objects for this page.
    index, row, col = 0, 0, 0
    cycles_data, instr_data = None, None
    outliers_exec, unique_exec, common_exec = None, None, None
    changepoint_exec, changepoint_mean_exec, changepoint_var_exec = None, None, None
    classification_exec = None
    while index < n_execs:
        data = executions[index]
        if cycles_executions:
            cycles_data = cycles_executions[index]
        if instr_executions:
            instr_data = instr_executions[index]
        if outliers:
            outliers_exec = outliers[index]
        if unique:
            unique_exec = unique[index]
        if common:
            common_exec = common[index]
        if changepoints:
            changepoint_exec = changepoints[index]
            classification_exec = classifications[index]
        if changepoint_means:
            changepoint_mean_exec = changepoint_means[index]
            changepoint_var_exec = changepoint_vars[index]
            classification_exec = classifications[index]
        # Get axis and draw plot.
        inner_grid = outer_grid[row, col]
        x_bounds = [xlimits_start, xlimits_stop]
        p_exec_charts.append(ProcessExecChart(inner_grid, data, cycles_data,
                 instr_data, instr_y_ranges, titles[index], x_bounds, [y_min, y_max],
                 y_range_zoom[index], window_size, outliers_exec, unique_exec,
                 common_exec, changepoint_exec, changepoint_mean_exec,
                 changepoint_var_exec, classification_exec, classifier,
                 core_cycles, cycles_ylimits, inset_x_bounds))
        p_exec_charts[index].plot_stack()
        col += 1
        if col == MAX_SUBPLOTS_PER_ROW:
            col = 0
            row += 1
        index = row * MAX_SUBPLOTS_PER_ROW + col
    # Update page grid before drawing insets, to ensure bounding boxes are correct.
    outer_grid.update(**SUBPLOT_PARAMS)
    # If the user has requested an inset and there are enough data points to
    # do so, plot insets on all charts.
    if inset and ((x_bounds[1] - x_bounds[0]) / 100.0) * 2.5 > 1:
        for chart in p_exec_charts:
            chart.plot_inset(chart.wallclock_axis, fig)
    if is_interactive:
        mng = pyplot.get_current_fig_manager()
        mng.resize(*mng.window.maxsize())
        pyplot.show()
        pyplot.close()
        return None, None
    else:
        # Return the figure to be saved in a multipage PDF.
        # Caller MUST close pyplot.
        return fig


def set_pdf_metadata(pdf_document):
    """Set metadata fields inside a PDF document.
    """
    info_dict = pdf_document.infodict()
    info_dict['Title'] = 'Krun results'
    info_dict['Author'] = 'soft-dev.org'
    info_dict['Creator'] = 'http://github.com/softdevteam/warmup_experiment'
    info_dict['Subject'] = 'Benchmarking results'
    info_dict['Keywords']= ('benchmark experiment interpreter measurement ' +
                            'software virtual machine')
    info_dict['CreationDate'] = datetime.datetime.today()
    info_dict['ModDate'] = datetime.datetime.today()


def get_data_dictionaries(json_files, benchmarks=[], wallclock_only=False,
                          outliers=False, unique_outliers=False, changepoints=False,
                          instr_dir=None):
    """Read a list of BZipped JSON files and return their contents as a
    dictionaries of key -> machine name -> results.

    This function returns ONLY the data that the user has requested on the
    command line. Therefore, we check carefully that all data can be found
    in the available Krun results files. Also, we pass back the title text
    for each subplot.
    """

    data_dictionary = {'data': dict(),  'cycles_counts': dict(),
                       'instr_data': dict(),  # Per-VM information.
                       'changepoints': dict(), 'changepoint_means': dict(),
                       'changepoint_vars': dict(), 'classifier': None,
                       'classifications': dict(), 'all_outliers': dict(),
                       'common_outliers': dict(), 'unique_outliers': dict(),
                      }

    plot_titles = dict()  # All subplot titles.
    window_size = None

    # Find out what data the user requested.
    # In bmark key -> machine  -> process execs format.
    requested_data = dict()
    if benchmarks != []:
        for quintuplet in benchmarks:
            try:
                machine, bmark, vm, variant, pexec = quintuplet.split(':')
            except ValueError:
                fatal_error('Malformed benchmark: %s. The correct '
                            'format is: machine:benchmark:vm:variant:pexec '
                            'e.g. mc.example.com:fasta:PyPy:default-python:0')
            key = ':'.join([bmark, vm, variant])
            if key not in requested_data:
                requested_data[key] = dict()
            if machine not in requested_data[key]:
                requested_data[key][machine] = list()
            requested_data[key][machine].append(int(pexec))

    # Collect the requested data from Krun results files.
    for filename in json_files:
        if not os.path.exists(filename):
            fatal_error('File %s does not exist.' % filename)
        print('Loading: %s' % filename)

        # All benchmarking data from one Krun results file.
        data = read_krun_results_file(filename)

        # Check that data requested on the command line exists in the JSON.
        if not wallclock_only and not ('core_cycle_counts' in data):
                fatal_error('Core cycle counts not stored in %s. '
                            'Consider running this script with --wallclock-only.'
                            % filename)
        if unique_outliers or outliers:
            if not ('common_outliers' in data and 'unique_outliers' in data and
                    'all_outliers' in data):
                fatal_error('You requested that outliers be annotated '
                            'on your plots, but file %s does not'
                            'contain the relevant keys. Please run the '
                            'mark_outliers_in_json.py script before '
                            'proceeding.' % filename)
        if changepoints:
            if 'changepoints' not in data:
                fatal_error('You requested that changepoints be annotated '
                            'on your plots, but file %s does not'
                            'contain the relevant keys. Please run the '
                            'mark_changepoints_in_json.py script before '
                            'proceeding.' % filename)
        data_has_changepoints = False  # Used for plot titles.
        if 'changepoints' in data:
            data_has_changepoints = True
            data_dictionary['classifier'] = data['classifier']

        # Get machine name from Krun results file.
        if ' ' in data['audit']['uname']:
            machine = data['audit']['uname'].split(' ')[1]
        else:
            machine = data['audit']['uname']
        if '.' in machine:  # Strip domain names.
            machine = machine.split('.')[0]
        machine_name = pretty_print_machine(machine)

        if 'instr_data' not in data_dictionary:
            data_dictionary['instr_data'] = dict()

        # Collect any results requested from this file.
        if benchmarks == []:  # Chart all available data from this file.
            for key in data['wallclock_times']:
                bmark = key.split(':')[0]
                if len(data['wallclock_times'][key]) == 0:
                    print('Skipping: %s:%s (no executions)' % (machine, key))
                else:
                    if key not in data_dictionary['data']:
                        data_dictionary['data'][key] = dict()
                        data_dictionary['cycles_counts'][key] = dict()
                        data_dictionary['instr_data'][key] = dict()
                        data_dictionary['changepoints'][key] = dict()
                        data_dictionary['changepoint_means'][key] = dict()
                        data_dictionary['changepoint_vars'][key] = dict()
                        data_dictionary['classifications'][key] = dict()
                        data_dictionary['all_outliers'][key] = dict()
                        data_dictionary['common_outliers'][key] = dict()
                        data_dictionary['unique_outliers'][key] = dict()
                    data_dictionary['data'][key][machine] = data['wallclock_times'][key]
                    print('Found: %s:%s (%d executions).' % (machine, key,
                                                             len(data['wallclock_times'][key])))
                    if wallclock_only:
                        data_dictionary['cycles_counts'][key][machine] = None
                        data_dictionary['instr_data'][key][machine] = None
                    else:
                        data_dictionary['cycles_counts'][key][machine] = data['core_cycle_counts'][key]
                        if instr_dir:
                            data_dictionary['instr_data'][key][machine] =  \
                                get_instr_data(
                                    key, machine, instr_dir,
                                    range(len(data['wallclock_times'][key])))
                        else:
                            data_dictionary['instr_data'][key][machine] = None
                    if changepoints:
                        data_dictionary['changepoints'][key][machine] = data['changepoints'][key]
                        data_dictionary['changepoint_means'][key][machine] = data['changepoint_means'][key]
                        data_dictionary['changepoint_vars'][key][machine] = data['changepoint_vars'][key]
                        data_dictionary['classifications'][key][machine] = data['classifications'][key]
                    else:
                        data_dictionary['changepoints'][key][machine] = None
                        data_dictionary['changepoint_means'][key][machine] = None
                        data_dictionary['changepoint_vars'][key][machine] = None
                        data_dictionary['classifications'][key][machine] = None
                    if outliers or unique_outliers:
                        data_dictionary['all_outliers'][key][machine] = data['all_outliers'][key]
                        data_dictionary['common_outliers'][key][machine] = data['common_outliers'][key]
                        data_dictionary['unique_outliers'][key][machine] = data['unique_outliers'][key]
                    else:
                        data_dictionary['all_outliers'][key][machine] = None
                        data_dictionary['common_outliers'][key][machine] = None
                        data_dictionary['unique_outliers'][key][machine] = None
            # Construct plot titles for all data in this file.
            for key in data_dictionary['data']:
                if key not in plot_titles:
                    plot_titles[key] = dict()
                if machine not in plot_titles[key]:
                    plot_titles[key][machine] = list()
                benchmark_name = key.split(':')[0]
                if benchmark_name in BENCHMARKS:
                    benchmark_name = BENCHMARKS[benchmark_name]
                else:
                    benchmark_name = benchmark_name.title()
                vm_name = key.split(':')[1]
                if vm_name in VMS:
                    vm_name = VMS[vm_name]

                # Add one title for each process execution
                try:
                    num_p_execs = len(data_dictionary['data'][key][machine])
                except KeyError:
                    pass  # no data for this
                else:
                    for p_exec in range(num_p_execs):
                        if data_has_changepoints:
                            classification = ' (%s)' % \
                                data['classifications'][key][p_exec]
                        else:
                            classification = ''
                        title = '%s, %s, %s, Proc. exec. #%d%s' % \
                                (benchmark_name,
                                 vm_name,
                                 machine_name,
                                 p_exec + 1,
                                 classification)
                        plot_titles[key][machine].append(title)
        else:  # Chart only the data specified on command line.
            skipped_keys = dict()
            for key in requested_data:
                if machine not in requested_data[key]:
                    continue
                if key not in data['wallclock_times']:
                    # Hope the key appears in another file, checked below.
                    continue
                if len(data['wallclock_times'][key]) == 0:
                    print('WARNING: Skipping: %s from %s (no executions)' % (key, machine))
                    if machine not in skipped_keys:
                        skipped_keys[machine] = list()
                    skipped_keys[machine].append(key)
                    continue

                # Scaffold entries if not existing
                if key not in data_dictionary['data']:
                    data_dictionary['data'][key] = dict()
                    data_dictionary['cycles_counts'][key] = dict()
                    data_dictionary['instr_data'][key] = dict()
                    data_dictionary['changepoints'][key] = dict()
                    data_dictionary['changepoint_means'][key] = dict()
                    data_dictionary['changepoint_vars'][key] = dict()
                    data_dictionary['all_outliers'][key] = dict()
                    data_dictionary['common_outliers'][key] = dict()
                    data_dictionary['unique_outliers'][key] = dict()
                    data_dictionary['classifications'][key] = dict()

                if machine not in data_dictionary['data'][key]:
                    data_dictionary['data'][key][machine] = list()
                    if not wallclock_only:
                        data_dictionary['cycles_counts'][key][machine] = list()
                        data_dictionary['instr_data'][key][machine] = list()
                    else:
                        data_dictionary['cycles_counts'][key][machine] = None
                        data_dictionary['instr_data'][key][machine] = None
                    if changepoints:
                        data_dictionary['changepoints'][key][machine] = list()
                        data_dictionary['changepoint_means'][key][machine] = list()
                        data_dictionary['changepoint_vars'][key][machine] = list()
                        data_dictionary['classifications'][key][machine] = list()
                    else:
                        data_dictionary['changepoints'][key][machine] = None
                        data_dictionary['changepoint_means'][key][machine] = None
                        data_dictionary['changepoint_vars'][key][machine] = None
                        data_dictionary['classifications'][key][machine] = None
                    if outliers or unique_outliers:
                        data_dictionary['all_outliers'][key][machine] = list()
                        data_dictionary['common_outliers'][key][machine] = list()
                        data_dictionary['unique_outliers'][key][machine] = list()
                    else:
                        data_dictionary['all_outliers'][key][machine] = None
                        data_dictionary['common_outliers'][key][machine] = None
                        data_dictionary['unique_outliers'][key][machine] = None

                if len(data['wallclock_times'][key]) > 0:
                    if key not in plot_titles:
                        plot_titles[key] = dict()
                    if machine not in plot_titles[key]:
                        plot_titles[key][machine] = list()
                    benchmark_name = key.split(':')[0]
                    if benchmark_name in BENCHMARKS:
                        benchmark_name = BENCHMARKS[benchmark_name]
                    else:
                        benchmark_name = benchmark_name.title()
                    vm_name = key.split(':')[1]
                    if vm_name in VMS:
                        vm_name = VMS[vm_name]
                    for p_exec in requested_data[key][machine]:
                        if p_exec >= len(data['wallclock_times'][key]):
                            fatal_error('You requested that process execution %g '
                                'for benchmark %s from machine %s be plotted, but '
                                'the Krun results file for that machine only has '
                                '%g process executions for the benchmark.' %
                                (p_exec, key, machine, len(data['wallclock_times'][key])))
                        # Add run sequence to data dictionary.
                        print('Adding run sequence to %s %s' % (key, machine))
                        data_dictionary['data'][key][machine].append(data['wallclock_times'][key][p_exec])
                        if not wallclock_only:
                            data_dictionary['cycles_counts'][key][machine].append(data['core_cycle_counts'][key][p_exec])
                            if instr_dir:
                                data_dictionary['instr_data'][key][machine].append(
                                    get_instr_data(key, machine, instr_dir, [p_exec])[0])
                            else:
                                data_dictionary['instr_data'][key][machine] = None
                        if changepoints:
                            data_dictionary['changepoints'][key][machine].append(data['changepoints'][key][p_exec])
                            data_dictionary['changepoint_means'][key][machine].append(data['changepoint_means'][key][p_exec])
                            data_dictionary['changepoint_vars'][key][machine].append(data['changepoint_vars'][key][p_exec])
                            data_dictionary['classifications'][key][machine].append(data['classifications'][key][p_exec])
                        if outliers or unique_outliers:
                            data_dictionary['all_outliers'][key][machine].append(data['all_outliers'][key][p_exec])
                            data_dictionary['common_outliers'][key][machine].append(data['common_outliers'][key][p_exec])
                            data_dictionary['unique_outliers'][key][machine].append(data['unique_outliers'][key][p_exec])

                        # Construct plot title.
                        if data_has_changepoints:
                            classification = ' (%s)' % \
                                data['classifications'][key][p_exec]
                        else:
                            classification = ''
                        title = '%s, %s, %s, Proc. exec. #%d%s' % \
                                (benchmark_name,
                                 vm_name,
                                 machine_name,
                                 p_exec + 1,
                                 classification)
                        plot_titles[key][machine].append(title)

    # Check that every benchmark that was requested has been found in the
    # given Krun results files.
    for key in requested_data:
        for machine in requested_data[key]:
            if machine in skipped_keys and key in skipped_keys[machine]:
                continue
            if (key not in data_dictionary['data'] or
                machine not in data_dictionary['data'][key]):
                fatal_error('You requested that plots for benchmark %s from '
                            'machine %s be produced, but no such data was '
                            'found in the Krun results files.' %
                            (key, machine))

    return data_dictionary, window_size, plot_titles


def create_cli_parser():
    """Create a parser to deal with command line switches.
    """
    script = os.path.basename(__file__)
    description = (('Plot data from Krun results file(s).'
                    '\n\nExample usage:\n\t$ python %s results1.json.bz2\n'
                    '\t$ python %s -i '
                    '--with-outliers results1.json.bz2 results2.json.bz2\n'
                    '\t$ python %s -b binarytrees:Hotspot:default-java'
                    ' results.json.bz2\n') %
                   (script, script, script))
    parser = argparse.ArgumentParser(description)
    parser.add_argument('json_files', nargs='+', action='append', default=[],
                        type=str, help='One or more Krun result files.')
    parser.add_argument('--instr-dir', action='store', default=None, type=str,
                        help='A directory containing VM instrumentation data.')
    parser.add_argument('--outfile', '-o', action='store', dest='outfile',
                        default=None, type=str,
                        help=('Name of the PDF file to write to. If no file is '
                              'specified, charts will be displayed interactively '
                              'and not written to disk.'))
    parser.add_argument('--wallclock-only', action='store_true', dest='wallclock',
                        default=False, help='Only plot wallclock times.')
    parser.add_argument('--benchmark', '-b', action='append', dest='benchmarks',
                        default=[], type=str,
                        help='Only draw charts for specific '
                             'machine:benchmark:vm:variant:pexec quintuplet(s). '
                             'e.g. "-b mc1.example.com:binarytrees:Hotspot:default-java:0. '
                             'will chart the first process execution of the '
                             'binary trees benchmark, in Java, on the Hotspot '
                             'VM, as measured on machine mc1. '
                             'This switch can be used repeatedly to chart '
                             'a number of benchmarks.')
    parser.add_argument('--one-page', action='store_true', dest='one_page',
                        default=False, help='Place all charts on a single page')
    parser.add_argument('--xlimits', '-x', action='store', dest='xlimits',
                        default=None, type=str,
                        help='Specify X-axis limits as a comma separated pair '
                             '\'start,end\'. Samples start from 0. e.g. '
                             '\'-x 100,130\' will show samples in the range '
                             '100 to 130.')
    parser.add_argument('--no-inset', action='store_true', dest='inset', default=False,
                        help='Do not place a small chart plotting the first few '
                             'values of each process execution in an thumbnail '
                             'inside each plot. The inset is intended to make it '
                             'easier to see detail during the warm-up phase of '
                             'each benchmark. Insets will not be drawn over '
                             'data measurements.')
    parser.add_argument('--no-zoom', action='store_true', dest='zoom', default=False,
                        help='Do not render a small chart showing a "zoomed in" '
                             'view of your measurements. This chart is intended '
                             'to show more detail in the "fast" iterations of '
                             'benchmarks with good warmup behaviour. ')
    parser.add_argument('--with-outliers', action='store_true', dest='outliers',
                        default=False,
                        help='Annotate outliers. Only use this if your '
                             'Krun results file contains outlier information.')
    parser.add_argument('--with-unique-outliers', action='store_true',
                        dest='unique_outliers', default=False,
                        help='Annotate outliers common to multiple '
                             'executions and those unique to single '
                             'executions differently. Only use this if '
                             'your Krun results file contains outlier '
                             'information.')
    parser.add_argument('--with-changepoints', action='store_true',
                        dest='changepoint_means', default=False,
                        help='Annotate changepoints and their means as vertical '
                             'and horizontal lines. Only use this if your Krun '
                             'results file contains changepoint information.')
    parser.add_argument('--export-size', action='store', default='12, 10',
                        help='Set the total size (comma separated pair, '
                             'in inches) of each process execution\'s '
                             'plots in the output.')
    parser.add_argument('--cycles-ylimits', action='store', default=None,
                        help='Set the y-axis range for core cycle plots '
                             '(comma separated pair)')
    parser.add_argument('--core-cycles', action='store', dest='core_cycles',
                        default=None,
                        help='Only plot the core cycles for specified cores '
                             '(comma separated). e.g: 0,1,2,3 or "" for no '
                             'plots. If this argument is not set, the default '
                             'behaviour is to plot as much data as is found in '
                             'the input files.')
    parser.add_argument('--inset-xlimits', '-X', action='store', dest='inset_xlimits',
                        default=None, type=str,
                        help='Similar to --xlimits, but for thumbnail plots.')
    return parser


def fatal_error(msg):
    print('')
    print('FATAL Krun plot error:')
    print('\t'+ msg)
    sys.exit(1)


if __name__ == '__main__':
    parser = create_cli_parser()
    options = parser.parse_args()
    if options.outliers and options.unique_outliers:
        fatal_error('Cannot use --with-outliers and --with-unique-outliers '
                    'together.')
    if options.outfile is None:
        pyplot.switch_backend('TkAgg')
    else:
        from matplotlib.backends.backend_pdf import PdfPages
        print('Saving results to: %s' % options.outfile)
    print('Using matplotlib version %s with backend %s' %
            (matplotlib.__version__, matplotlib.get_backend()))
    print('Charting with xlimits: %s' % options.xlimits)

    # Set process execution plot-stack sizes
    try:
        sz_x, sz_y = options.export_size.split(',')
        EXPORT_SIZE_INCHES[0] = float(sz_x)
        EXPORT_SIZE_INCHES[1] = float(sz_y)
    except ValueError:
        fatal_error('invalid --export-size argument')

    if options.cycles_ylimits and options.wallclock:
        fatal_error('Cannot use --cycles-ylimits AND --wallclock-only.')

    if options.cycles_ylimits:
        try:
            y1, y2 = options.cycles_ylimits.split(',')
            cycles_ylimits = float(y1), float(y2)
        except ValueError:
            fatal_error('invalid --cycles-ylimits argument')
    else:
        cycles_ylimits = None

    if options.core_cycles and options.wallclock:
        fatal_error('Cannot use --core-cycles AND --wallclock-only.')

    if options.core_cycles is not None and not options.wallclock:
        if options.core_cycles.strip() == "":
            core_cycles = list()  # Plot no CC data.
        else:
            try:
                cycles_str = options.core_cycles.split(',')
                core_cycles = [int(cycle) for cycle in cycles_str]
            except ValueError:
                fatal_error('invalid --core-cycles argument')
            print('Plotting cycle counts for core(s): %s' % ','.join([str(core) for core in core_cycles]))
    else:
        core_cycles = None  # Default: plot all data in input file.
        print('Plotting all available cycle counts data.')

    if options.xlimits:
        try:
            options.xlimits = [int(x) for x in options.xlimits.split(',')]
        except ValueError:
            fatal_error('invalid --xlimits pair')

        if len(options.xlimits) != 2:
            fatal_error('wrong number of --xlimits elements')

    if options.inset_xlimits:
        try:
            options.inset_xlimits = \
                [int(x) for x in options.inset_xlimits.split(',')]
        except ValueError:
            fatal_error('invalid --inset-xlimits pair')

        if len(options.inset_xlimits) != 2:
            fatal_error('wrong number of --inset-xlimits elements')

    if options.xlimits and options.inset_xlimits:
        if options.inset_xlimits[0] < options.xlimits[0] or \
            options.inset_xlimits[1] > options.xlimits[1]:
            fatal_error('--inset-xlimits range must be inside --xlimits range')

    if not options.instr_dir:
        print('No VM instrumentation data is available.')
    else:
        if not os.path.exists(options.instr_dir):
            fatal_error('%s (VM instrumentation data directory) does not exist.' %
                        options.instr_dir)
        elif not os.path.isdir(options.instr_dir):
            fatal_error('%s (VM instrumentation data directory) is not a directory.' %
                        options.instr_dir)
        else:
            print('Collecting instrumentation data for from %s.' % options.instr_dir)

    # Smaller fonts for on-screen plots.
    if options.outfile is None:
        TICK_FONTSIZE = 12
        TITLE_FONT_SIZE = 17
        AXIS_FONTSIZE = 14
        BASE_FONTSIZE = 13

    data, window_size, plot_titles = get_data_dictionaries(options.json_files[0],
                                        options.benchmarks, options.wallclock,
                                        options.outliers, options.unique_outliers,
                                        options.changepoint_means,
                                        options.instr_dir)
    if window_size:
        print('Data generated with window size: %d.' % window_size)

    # Find the number of in-proc iterations in a non-crashed pexec
    # Assumes we use the same number of in-proc iterations for all pexecs.
    try:
        for key_data in data['data'].values():
            for bench_data in key_data.values():
                for pexec in bench_data:
                    if pexec == []:
                        # indicates a crash
                        continue
                    else:
                        iter_lens = len(pexec)
                        raise StopIteration()  # to break out of all loops at once
        else:
            fatal_error('Could not find a non-crashing pexec')
    except StopIteration:
        pass  # good, we found some non-crash data

    if window_size and window_size >= iter_lens:
        fatal_error('Window size (%d) must be strictly less than the number '
                    'of iterations in each process execution (%d). Use '
                    '-w or --window on the command line.' %
                    (window_size, iter_lens))

    main(options.outfile is None,
         data,
         plot_titles,
         window_size,
         options.outfile,
         options.xlimits,
         options.outliers,
         options.unique_outliers,
         options.changepoint_means,
         inset=not options.inset,
         zoom=not options.zoom,
         one_page=options.one_page,
         core_cycles=core_cycles,
         cycles_ylimits=cycles_ylimits,
         inset_xlimits=options.inset_xlimits)
