#!/usr/bin/env python3
#
# Copyright (c) 2017 King's College London
# created by the Software Development Team <http://soft-dev.org/>
#
# The Universal Permissive License (UPL), Version 1.0
#
# Subject to the condition set forth below, permission is hereby granted to any
# person obtaining a copy of this software, associated documentation and/or
# data (collectively the "Software"), free of charge and under any and all
# copyright rights in the Software, and any and all patent rights owned or
# freely licensable by each licensor hereunder covering either (i) the
# unmodified Software as contributed to or provided by such licensor, or (ii)
# the Larger Works (as defined below), to deal in both
#
# (a) the Software, and
# (b) any piece of software and/or hardware listed in the lrgrwrks.txt file if
# one is included with the Software (each a "Larger Work" to which the Software
# is contributed by such licensors),
#
# without restriction, including without limitation the rights to copy, create
# derivative works of, display, perform, and distribute the Software and make,
# use, sell, offer for sale, import, export, have made, and have sold the
# Software and the Larger Work(s), and to sublicense the foregoing rights on
# either these or other terms.
#
# This license is subject to the following condition: The above copyright
# notice and either this complete permission notice or at a minimum a reference
# to the UPL must be included in all copies or substantial portions of the
# Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.
"""Top-level wrapper script for summarising benchmark results."""

import os
import sys

# R packages are stored relative to the top-level of the repo.
if ('R_LIBS_USER' not in os.environ or 'rlibs' not in os.environ['R_LIBS_USER']):
    os.environ['R_LIBS_USER'] = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))),
                                             'work', 'rlibs')
    args = [sys.executable]
    args.extend(sys.argv)
    os.execv(sys.executable, args)

import argparse
import json
import logging
import os.path
import subprocess

from distutils.spawn import find_executable
from logging import debug, error, info, warn
sys.path.insert(0, os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from warmup.krun_results import csv_to_krun_json, parse_krun_file_with_changepoints
from warmup.krun_results import read_krun_results_file
from warmup.summary_statistics import collect_summary_statistics, convert_to_latex
from warmup.summary_statistics import write_html_table, write_latex_table


# We use a custom install of rpy2, relative to the top-level of the repo.
sys.path.insert(0, os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))),
                                'work', 'pylibs'))

ABS_TIME_FORMAT = '%Y-%m-%d %H:%M:%S'
BINDIR = os.path.dirname(os.path.abspath(__file__))
DEFAULT_WINDOW_RATIO = 0.1
DEFAULT_STEADY_RATIO = 0.25
SCRIPT_DIFF_RESULTS = os.path.join(BINDIR, 'diff_results')
SCRIPT_MARK_OUTLIERS = os.path.join(BINDIR, 'mark_outliers_in_json')
SCRIPT_MARK_CHANGEPOINTS = os.path.join(BINDIR, 'mark_changepoints_in_json')
SCRIPT_PLOT_KRUN_RESULTS = os.path.join(BINDIR, 'plot_krun_results')

CONSOLE_FORMATTER = PLAIN_FORMATTER = logging.Formatter(
    '[%(asctime)s: %(levelname)s] %(message)s',
    ABS_TIME_FORMAT)
try:
    import colorlog
    CONSOLE_FORMATTER = colorlog.ColoredFormatter(
        "%(log_color)s[%(asctime)s %(levelname)s] %(message)s%(reset)s",
        ABS_TIME_FORMAT)
except ImportError:
    pass


CSV_COMBO_MSG = 'Note that CSV files should contain data for EXACTLY one language / VM combination.'

DESCRIPTION = lambda fname: """
Analyse CSV or Krun results file(s) and produce a JSON summary, result table,
result plots or diff table. Tables of results or diffs can be produced in HTML
or LaTeX/PDF.

Input files may be in the following CSV format:

    process num, bench_name, 0, 1, 2, ...
    0, spectral norm, 0.2, 0.1, 0.4, ...
    1, spectral norm, 0.3, 0.15, 0.2, ...

or be json.bz2 files, output by the Krun(*) tool.
(*) Krun: http://soft-dev.org/src/krun/

%s

Example usage - output JSON summary:

    $ python %s --output-json summary.json -l javascript -v V8 -u "`uname -a`" results.csv

Example usage - output HTML table:

    $ python %s --html --output-table results.html -l javascript -v V8 -u "`uname -a`" results.json.bz2

Example usage - output PDF plot:

    $ python %s --output-plots plots.pdf -l javascript -v V8 -u "`uname -a`" results.csv

Example usage - output PDF plot with VM instrumentation data:

    $ python %s --output-plots plots.pdf --instr-dir vm_instr_data/ results.json.bz2

Example usage - output LaTeX/PDF diff:

    $ python %s --tex --output-diff diff.tex -l javascript -v V8 -u "`uname -a`" before.csv after.csv
""" % (CSV_COMBO_MSG, fname, fname, fname, fname, fname)


def fatal(msg):
    """Log error and exit."""

    error(msg)
    sys.exit(1)


def create_arg_parser():
    parser = argparse.ArgumentParser(description=DESCRIPTION(os.path.basename(__file__)),
                                     formatter_class=argparse.RawTextHelpFormatter)
    parser.add_argument('input_files', nargs='+', action='append', default=[],
                        type=str, help='One or more CSV or Krun results files.')
    parser.add_argument('--debug', '-d', action='store', default='WARN',
                        dest='debug_level',
                        help='Debug level used by logger. Must be one of: '
                             'DEBUG, INFO, WARN, DEBUG, CRITICAL, ERROR')
    csv_only_msg = 'Only needed if input files are in CSV format.'
    parser.add_argument('--language', '-l', dest='language', action='store',
                        type=str, help=('Language under test (in lower-case). %s %s'
                                        % (csv_only_msg, CSV_COMBO_MSG)))
    parser.add_argument('--vm', '-v', dest='vm', action='store', type=str,
                        help=('Virtual machine under test (in title-case). %s %s'
                              % (csv_only_msg, CSV_COMBO_MSG)))
    parser.add_argument('--uname', '-u', dest='uname', action='store', default='',
                        type=str, help=('Full output of `uname -a` from benchmarking machine. '
                                        'Only needed if input files are in CSV format.'))
    parser.add_argument('--instr-dir', dest='instr_dir', action='store', default='',
                        type=str, help=('Directory containing instrumentation data. '
                                        'Only useful when generating plots.'))
    parser.add_argument('--diff-vms', action='append', nargs=2, dest='diff_vms', default=[],
                         help='Compare one VM against another. \nRequires two '
                              'VM names as arguments. By default, the\ndiffer '
                              'will compare all benchmarks / VMs which appear\n'
                              'in both input files.\nOnly makes sense with --output-diff.')
    # What output file format should be generated?
    format_group = parser.add_mutually_exclusive_group(required=False)
    format_group.add_argument('--html', dest='type_html', action='store_true', default=False,
                              help=('Output an HTML file. Valid with --output-table '
                                    'and --output-diff.'))
    format_group.add_argument('--tex', dest='type_latex', action='store_true', default=False,
                              help=('Output a LaTeX file and convert to PDF. Valid '
                                    'with --output-table and --output-diff.'))
    # What output should warmup_stats generate?
    output_group = parser.add_mutually_exclusive_group(required=True)
    output_group.add_argument('--output-plots', dest='output_plots', action='store',
                              type=str, metavar='PDF_FILENAME', default=None,
                              help='Output a PDF file containing plots (HTML unavailable).')
    output_group.add_argument('--output-table', dest='output_table', action='store',
                              type=str, metavar='TABLE_FILENAME', default=None,
                              help='Output a file containing table. Requires --tex or --html.')
    output_group.add_argument('--output-json', dest='output_json', action='store',
                              type=str, metavar='JSON_FILENAME', default=None,
                              help='Output a JSON file containing a statistical summary.')
    output_group.add_argument('--output-diff', dest='output_diff', action='store',
                              type=str, metavar='DIFF_FILENAME', default=None,
                              help='Output a file containing a diff table. Requires '
                              '--tex or --html. Expects exactly two input files.')
    return parser


def setup_logging(options):
    """Setup logging. Logging level passed in on command line."""

    level_str = options.debug_level.upper()
    if level_str not in ('DEBUG', 'INFO', 'WARN', 'DEBUG', 'CRITICAL', 'ERROR'):
        fatal('Bad debug level: %s' % level_str)
    level = getattr(logging, level_str.upper())
    logging.root.setLevel(level)
    stream = logging.StreamHandler()
    stream.setLevel(level)
    stream.setFormatter(CONSOLE_FORMATTER)
    logging.root.handlers = [stream]


def check_environment(need_outliers=True, need_changepoints=True, need_latex=True,
                      need_plots=True):
    """Check all modules or executables that the user needs will be available."""

    info('Checking environment.')
    python_path = None
    pypy_path = None
    pdflatex_path = None
    r_path = None
    python_path = find_executable('python3')
    if python_path is None:
        fatal('warmup scripts require Python 3')
    if find_executable('bzip2') is None or find_executable('bunzip2') is None:
        fatal('Please install bzip2 and bunzip2 to convert CSV files to Krun JSON format.')
    if need_outliers:
        pypy_path = find_executable('pypy')
        if pypy_path is None:
            warn('You do not appear to have PyPy installed. Some parts of this '
                 'script may run slowly.')
    if need_changepoints or need_plots:
        try:
            import numpy
        except ImportError:
            fatal('Please install the Python numpy library to generate changepoints and / or plots.')
    if need_changepoints:
        r_path = find_executable('R')
        if r_path is None:
            fatal('Please install R (e.g. r-base) to generate changepoints.')
        try:
            import rpy2
        except ImportError:
            fatal('Please install the Python rpy2 library to generate changepoints.')
    if need_latex:
        pdflatex_path = find_executable('pdflatex')
        if pdflatex_path is None:
            fatal('You do not appear to have pdflatex installed, and so cannot '
                  'compile LaTeX files. Please install LaTeX (e.g. tetex-live) and try again.')
    if need_plots:
        try:
            import matplotlib
        except ImportError:
            fatal('Please install the Python matplotlib library, with Agg backend, to generate plots.')
        try:
            matplotlib.use('Agg')
        except:
            fatal('Python matplotlib is installed, but the Agg backend is also needed to generate plots.')
    return python_path, pypy_path, pdflatex_path, r_path


class BenchmarkFile(object):
    """Information about each benchmark, including CLI options to other scripts."""

    def __init__(self, filename, options, python_path, pypy_path, pdflatex_path, r_path):
        self.basename = os.path.splitext(filename)[0]
        self.language = options.language
        self.vm = options.vm
        self.uname = options.uname
        self.python_path = python_path
        self.pypy_path = pypy_path
        self.pdflatex_path = pdflatex_path
        self.r_path = r_path
        self.csv_filename = None
        self.krun_filename = None
        self.krun_filename_outliers = None
        self.krun_filename_changepoints = None
        assert filename.endswith('.csv') or filename.endswith('.json.bz2'), \
            'Unknown file type: %s. Please use CSV or Krun output.' % filename
        if filename.endswith('.csv'):
            self.csv_filename = filename
            # self.iterations is set in self.convert_to_krun_json().
        if filename.endswith('_changepoints.json.bz2'):
            self.krun_filename_changepoints = filename
        if '_outliers_w' in filename:
            self.krun_filename_outliers = filename
        if filename.endswith('.json.bz2'):
            self.krun_filename = filename
            if not os.path.exists(filename):
                fatal('File %s does not exist.' % filename)
            # If the input file comes from Krun, we must still set
            # self.iterations, which is needed by other methods. We assume the
            # same number of iterations for all pexecs.
            data = read_krun_results_file(filename)
            found_full_pexec = False
            if 'window_size' in data:
                self.window = data['window_size']
            for bench in data['wallclock_times']:
                if found_full_pexec:
                    break
                for pexec in data['wallclock_times'][bench]:
                    if pexec == []:  # Crashed benchmark.
                        continue
                    else:
                        self.iterations = len(pexec)
                        debug('%d iterations per pexec in %s.' % (self.iterations, filename))
                        found_full_pexec = True
                        break
            if not found_full_pexec:
                fatal('Could not find a non-crashing pexec in %s.' % filename)

    def check_input_file(self):
        if not self.csv_filename and not (os.path.isfile(self.krun_filename) and
                                          os.access(self.krun_filename, os.R_OK)):
            fatal('File %s not found.' % self.krun_filename)
        if self.csv_filename and not (os.path.isfile(self.csv_filename) and
                                      os.access(self.csv_filename, os.R_OK)):
            fatal('File %s not found.' % self.csv_filename)

    def convert_to_krun_json(self):
        if self.krun_filename is not None:
            debug('Krun file already exists: %s' % self.krun_filename)
            return
        header, self.krun_filename = csv_to_krun_json([self.csv_filename],
                                             self.language, self.vm, self.uname)
        info('Writing out: %s' % self.krun_filename)
        try:
            self.iterations = int(header[-1]) + 1  # Iteration numbers start at 0.
        except ValueError:
            fatal('CSV file has malformed header. Run this script with --help for more details.')

    def _get_output_filename(self, output):
        for line in output.strip().decode().split('\n'):
            if line.startswith('Writing out:'):
                return line.split(' ')[-1]
        assert False

    def mark_outliers(self):
        if self.krun_filename_outliers is not None:
            debug('Krun file already has outliers: %s' % self.krun_filename_outliers)
            return
        self.window = int(self.iterations * DEFAULT_WINDOW_RATIO)
        python_runner = self.python_path
        # mark_outliers_in_json is optimised for PyPy.
        if self.pypy_path is not None:
            python_runner = self.pypy_path
        cli = [python_runner, SCRIPT_MARK_OUTLIERS, '-w', str(self.window), self.krun_filename]
        debug('Running: %s' % ' '.join(cli))
        output = subprocess.check_output(' '.join(cli), shell=True)
        self.krun_filename_outliers = self._get_output_filename(output)
        debug('Written out: %s' % self.krun_filename_outliers)

    def mark_changepoints(self):
        if self.krun_filename_changepoints is not None:
            debug('Krun file already has changepoints: %s' % self.krun_filename_changepoints)
            return
        self.steady = int(self.iterations * DEFAULT_STEADY_RATIO)
        cli = [self.python_path, SCRIPT_MARK_CHANGEPOINTS, '-s', str(self.steady),
               self.krun_filename_outliers]
        debug('Running: %s' % ' '.join(cli))
        output = subprocess.check_output(' '.join(cli), shell=True)
        self.krun_filename_changepoints = self._get_output_filename(output)
        debug('Written out: %s' % self.krun_filename_changepoints)


def main(options):
    info('Checking sanity of CLI options.')
    need_latex = (options.output_table or options.output_diff) and options.type_latex
    need_plots = options.output_plots
    if options.output_table and not (options.type_latex or options.type_html):
        fatal('--output-table must be used with either --html or --tex.')
    if options.output_diff and not (options.type_latex or options.type_html):
        fatal('--output-diff must be used with either --html or --tex.')
    if options.diff_vms and not options.output_diff:
        fatal('--diff-vms must be used with --output-diff.')
    input_files = options.input_files[0]
    for filename in input_files:
        if filename.endswith('.csv'):
            if not options.language:
                fatal('--language or -l must be used with CSV input files.')
            if not options.vm:
                fatal('--vm or -v must be used with CSV input files.')
            if not options.uname:
                fatal('--uname or -u must be used with CSV input files.')
    if options.output_diff and len(input_files) != 2:
        fatal('--output-diff expects exactly 2 input files.')
    if options.instr_dir:
        if not os.path.exists(options.instr_dir):
            fatal('%s (VM instrumentation data directory) does not exist.' % options.instr_dir)
        elif not os.path.isdir(options.instr_dir):
            fatal('%s (VM instrumentation data directory) is not a directory.' % options.instr_dir)
        else:
            debug('Collecting instrumentation data for from %s.' % options.instr_dir)
    else:
        debug('No VM instrumentation data is available.')
    python_path, pypy_path, pdflatex_path, r_path = check_environment(need_latex=need_latex,
                                                                      need_plots=need_plots)
    info('Processing input files, converting to Krun JSON if necessary.')
    benchmarks = list()
    for filename in input_files:
        if not (filename.endswith('.csv') or filename.endswith('json.bz2')):
            fatal('Cannot determine filetype of %s. Please use .csv or .json.bz2 (Krun) files only.' % filename)
        benchmarks.append(BenchmarkFile(filename, options, python_path, pypy_path, pdflatex_path, r_path))
    info('Checking input files.')
    for benchmark in benchmarks:
        benchmark.check_input_file()
    info('Converting CSV to Krun JSON.')
    for benchmark in benchmarks:
        if benchmark.csv_filename:
            benchmark.convert_to_krun_json()
    info('Marking outliers in JSON.')
    for benchmark in benchmarks:
        if not benchmark.krun_filename_outliers:
            benchmark.mark_outliers()
    info('Marking changepoints in JSON.')
    for benchmark in benchmarks:
        if not benchmark.krun_filename_changepoints:
            benchmark.mark_changepoints()
    # Generate appropriate output.
    if options.output_diff and options.type_latex:
        info('Generating LaTeX diff table.')
        input_files = [bm.krun_filename_changepoints for bm in benchmarks]
        assert len(input_files) == 2
        if options.diff_vms:
            cli = [python_path, SCRIPT_DIFF_RESULTS, '--tex', options.output_diff,
                   '--input-results', ' '.join(input_files), '--vm',
                   options.diff_vms[0][0], options.diff_vms[0][1]]
        else:
            cli = [python_path, SCRIPT_DIFF_RESULTS, '--tex', options.output_diff,
                   '--input-results', ' '.join(input_files)]
        debug('Running: %s' % ' '.join(cli))
        output = subprocess.check_output(' '.join(cli), shell=True)
        for line in output.strip().split('\n'):
            if line.startswith('Writing data to:'):
                debug('Written out: %s' % line.split(' ')[-1])
        info('Compiling diff table as PDF.')
        cli = [pdflatex_path, '-interaction=batchmode', options.output_diff]
        debug('Running: %s' % ' '.join(cli))
        subprocess.check_output(' '.join(cli), shell=True)
        subprocess.check_output(' '.join(cli), shell=True)
    if options.output_diff and options.type_html:
        info('Generating HTML diff table.')
        input_files = [bm.krun_filename_changepoints for bm in benchmarks]
        assert len(input_files) == 2
        if options.diff_vms:
            cli = [python_path, SCRIPT_DIFF_RESULTS, '--html', options.output_diff,
                       '--input-results', ' '.join(input_files), '--vm',
                       options.diff_vms[0][0], options.diff_vms[0][1]]
        else:
            cli = [python_path, SCRIPT_DIFF_RESULTS, '--html', options.output_diff,
                   '--input-results', ' '.join(input_files)]
        debug('Running: %s' % ' '.join(cli))
        output = subprocess.check_output(' '.join(cli), shell=True)
        for line in output.strip().split('\n'):
            if line.startswith('Writing data to:'):
                debug('Written out: %s' % line.split(' ')[-1])
    if options.output_json or options.output_table:
        info('Collecting summary statistics.')
        input_files = [bm.krun_filename_changepoints for bm in benchmarks]
        classifier, data_dictionary = parse_krun_file_with_changepoints(input_files)
        summary = collect_summary_statistics(data_dictionary, classifier['delta'], classifier['steady'])
    if options.output_plots:
        info('Generating PDF plots.')
        input_files = [bm.krun_filename_changepoints for bm in benchmarks]
        iterations = benchmarks[0].iterations
        if len(benchmarks) > 1:
            for bm in benchmarks:
                if bm.iterations != iterations:
                    sys.stderr.write('File %s contains pexecs with %d iterations, expected %d. '
                                     'All process executions in all files should compute the '
                                     'same number of iterations.' %
                                     (bm.csv_filename, bm.iterations, iterations))
                    sys.exit(1)
        if options.instr_dir:
            cli = [python_path, SCRIPT_PLOT_KRUN_RESULTS, '--with-changepoints',
                   '--with-outliers', '-o', options.output_plots,
                   '--instr-dir', options.instr_dir, ' '.join(input_files)]
        else:
            cli = [python_path, SCRIPT_PLOT_KRUN_RESULTS, '--with-changepoints',
                   '--with-outliers', '-o', options.output_plots,
                   ' '.join(input_files)]
        debug('Running: %s' % ' '.join(cli))
        subprocess.check_output(' '.join(cli), shell=True)
        debug('Written out: %s' % options.output_plots)
    if options.output_json:
        info('Generating JSON.')
        with open(options.output_json, 'w') as fd:
            json.dump(summary, fd, sort_keys=True, ensure_ascii=True, indent=4)
        debug('Written out: %s' % options.output_json)
    if options.output_table and options.type_latex:
        info('Generating LaTeX / PDF table.')
        machine, bmarks, latex_summary = convert_to_latex(summary, classifier['delta'], classifier['steady'])
        write_latex_table(machine, bmarks, latex_summary, options.output_table,
                          longtable=True, with_preamble=True)
        info('Compiling table as PDF.')
        cli = [pdflatex_path, '-interaction=batchmode', options.output_table]
        debug('Running: %s' % ' '.join(cli))
        subprocess.check_output(' '.join(cli), shell=True)
        subprocess.check_output(' '.join(cli), shell=True)
    if options.output_table and options.type_html:
        info('Generating HTML table.')
        write_html_table(summary, options.output_table)


if __name__ == '__main__':
    parser = create_arg_parser()
    options = parser.parse_args()
    setup_logging(options)
    debug('%s script starting...' % os.path.basename(__file__))
    debug('arguments: %s'  % ' '.join(sys.argv[1:]))
    main(options)
